# Awesome colab notebooks collection for ML experiments
## Research
| name | description | authors | links | colaboratory | update |
|------|-------------|:--------|:------|:------------:|:------:|
| CLIPDraw | Synthesize drawings to match a text prompt | <ul><li>[Kevin Frans](https://www.kvfrans.com/)</li> <li>[Lisa Soros](https://scholar.google.com/citations?user=iUkpvMUAAAAJ)</li> <li>[Olaf Witkowski](https://olafwitkowski.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.14843), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.06576), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.00162)</li><li>[blog post](https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/kvfrans/clipdraw), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/BachiLi/diffvg/blob/master/apps/painterly_rendering.py)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb) | 28.04.2022 |
| YOLOv5 | You Only Look Once | [Glenn Jocher](https://github.com/glenn-jocher) | <ul><li>[data](http://cocodataset.org/#upload)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ultralytics/yolov5)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/yolov5), [<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/coco128)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb) | 26.04.2022 |
| Real-ESRGAN | Extend the powerful ESRGAN to a practical restoration application, which is trained with pure synthetic data | <ul><li>[Xintao Wang](https://xinntao.github.io/)</li> <li>[Liangbin Xie](https://github.com/LiangbinXie)</li> <li>[Chao Dong](https://scholar.google.com/citations?user=OSDCB0UAAAAJ)</li> <li>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.10833)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/Real-ESRGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/ESRGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/facexlib), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/HandyView), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Tencent/ncnn), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/nihui/waifu2x-ncnn-vulkan)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo) | 24.04.2022 |
| Customizing a Transformer Encoder | We will learn how to customize the encoder to employ new network architectures | [Chen Chen](https://github.com/chenGitHuber) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.03762)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/models/tree/master/official/nlp/modeling), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/encoder_scaffold.py)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/nlp/customize_encoder.ipynb) | 22.04.2022 |
| Demucs | Hybrid Spectrogram and Waveform Source Separation | [Alexandre Défossez](https://ai.honu.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.03600), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.01733), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.05418), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.02410)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/demucs/), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/adefossez/mdx21_demucs), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/CarlGao4/Demucs-Gui), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/kuielab/mdx-net-submission), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/f90/Wave-U-Net)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1dC9nVxk3V_VPjUADsnFu8EiT-xnU1tGH) | 23.03.2022 |
| Parallel WaveGAN | State-of-the-art non-autoregressive models to build your own great vocoder | [Tomoki Hayashi](https://kan-bayashi.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.11480), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.06711), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.05106)</li><li>[demo](https://kan-bayashi.github.io/ParallelWaveGAN/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/kan-bayashi/ParallelWaveGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/tacotron2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/espnet/espnet)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb) | 17.03.2022 |
| AlphaFold | Highly accurate protein structure prediction | <ul><li>[John Jumper](https://scholar.google.com/citations?user=a5goOh8AAAAJ)</li> <li>[Richard Evans](http://www.doc.ic.ac.uk/~re14/)</li> <li>[Alexander Pritzel](https://scholar.google.com/citations?user=GPgAyU0AAAAJ)</li> <li>[Tim Green](http://tfgg.me/)</li> <li>[Michael Figurnov](https://figurnov.ru/)</li> <li>[Olaf Ronneberger](https://lmb.informatik.uni-freiburg.de/people/ronneber/)</li> <li>[Kathryn Tunyasuvunakool](https://scholar.google.com/citations?user=eEqNGagAAAAJ)</li> <li>[Russ Bates](https://scholar.google.com/citations?user=Koes5ewAAAAJ)</li> <li>[Augustin Žídek](https://augustin.zidek.eu/)</li> <li>[Anna Potapenko](http://apotapenko.com/)</li> <li>[Alex Bridgland](https://scholar.google.com/citations?user=VWmXKPMAAAAJ)</li> <li>[Clemens Meyer](https://scholar.google.com/citations?user=EWLZiM8AAAAJ)</li> <li>[Simon Kohl](https://www.simonkohl.com/)</li> <li>[Andrew Ballard](https://scholar.google.com/citations?user=syjQhAMAAAAJ)</li> <li>[Bernardino Romera-Paredes](https://scholar.google.com/citations?user=_LC9U6EAAAAJ)</li> <li>[Stanislav Nikolov](https://scholar.google.co.uk/citations?user=O-b7pBEAAAAJ)</li> <li>[Rishub Jain](http://rishub.me/)</li></ul> | <ul><li>[blog post](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology), [blog post](https://deepmind.com/blog/article/putting-the-power-of-alphafold-into-the-worlds-hands)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/alphafold/), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/tree), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/chex)</li><li>[paper](https://www.nature.com/articles/s41586-021-03819-2), [paper](https://www.nature.com/articles/s41586-021-03828-1)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/alphafold)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/AlphaFold)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=gg7WjuFs8F4), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=B9PL__gVxLI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/deepmind/alphafold/blob/master/notebooks/AlphaFold.ipynb) | 16.03.2022 |
| VideoGPT | A conceptually simple architecture for scaling likelihood based generative modeling to natural videos | <ul><li>[Wilson Yan](https://wilson1yan.github.io/)</li> <li>[Yunzhi Zhang](https://zzyunzhi.github.io/)</li> <li>[Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)</li> <li>[Aravind Srinivas](https://people.eecs.berkeley.edu/~aravind/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.10157), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.10509)</li><li>[data](https://www.crcv.ucf.edu/data/UCF101.php)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/wilson1yan/VideoGPT)</li><li>[project](https://wilson1yan.github.io/videogpt/index.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/wilson1yan/VideoGPT/blob/master/notebooks/Using_VideoGPT.ipynb) | 02.03.2022 |
| MTTR | End-to-End Referring Video Object Segmentation with Multimodal Transformers | <ul><li>[Adam Botach](https://www.linkedin.com/in/adam-botach)</li> <li>[Evgenii Zheltonozhskii](https://evgeniizh.com/)</li> <li>[Chaim Baskin](https://github.com/chaimbaskin)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.14821), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.11692), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.13230)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mttr2021/MTTR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/SwinTransformer/Video-Swin-Transformer)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/12p0jpSx3pJNfZk-y_L44yeHZlhsKVra-) | 28.02.2022 |
| VRT | A Video Restoration Transformer | <ul><li>[Jingyun Liang](https://jingyunliang.github.io/)</li> <li>[Jiezhang Cao](https://github.com/caojiezhang)</li> <li>[Yuchen Fan](https://ychfan.github.io/)</li> <li>[Kai Zhang](https://cszn.github.io/)</li> <li>[Yawei Li](https://ofsoundof.github.io/)</li> <li>[Radu Timofte](http://people.ee.ethz.ch/~timofter/)</li> <li>[Luc Van Gool](https://scholar.google.com/citations?user=TwMib_QAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.12288)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/JingyunLiang/VRT), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/cszn/KAIR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/SwinTransformer/Video-Swin-Transformer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/open-mmlab/mmediting)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/gist/JingyunLiang/deb335792768ad9eb73854a8efca4fe0/vrt-demo-on-video-restoration.ipynb) | 25.02.2022 |
| Disentangled Lifespan Face Synthesis | LFS model is proposed to disentangle the key face characteristics including shape, texture and identity so that the unique shape and texture age transformations can be modeled effectively | <ul><li>[Sen He](https://senhe.github.io/)</li> <li>[Wentong Liao](https://www.tnt.uni-hannover.de/en/staff/liao/)</li> <li>[Michael Yang](https://sites.google.com/site/michaelyingyang/)</li> <li>[Yi-Zhe Song](http://personal.ee.surrey.ac.uk/Personal/Y.Song/)</li> <li>[Bodo Rosenhahn](https://scholar.google.com/citations?user=qq3TxtcAAAAJ)</li> <li>[Tao Xiang](http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.02874)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/SenHe/DLFS)</li><li>[project](https://senhe.github.io/projects/iccv_2021_lifespan_face/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=uklX03ns0m0)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1fgVAoxCSaqPkj0rUK4RmBh7GTQRqLNpE) | 22.02.2022 |
| ArcaneGAN | Process video in the style of the Arcane animated series | [Alexander Spirin](https://github.com/Sxela) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Sxela/ArcaneGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Sxela/stylegan3_blending)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Fi199uFW6jE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/AJG4X7IokG8)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1r1hhciakk5wHaUn1eJk7TP58fV9mjy_W) | 17.02.2022 |
| GFPGAN | Towards Real-World Blind Face Restoration with Generative Facial Prior | <ul><li>[Xintao Wang](https://xinntao.github.io/)</li> <li>[Yu Li](https://yu-li.github.io/)</li> <li>[Honglun Zhang](https://scholar.google.com/citations?user=KjQLROoAAAAJ)</li> <li>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2101.04061)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/TencentARC/GFPGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/facexlib), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/HandyView), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset)</li><li>[project](https://xinntao.github.io/projects/gfpgan)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo) | 14.02.2022 |
| SpecVQGAN | Taming the visually guided sound generation by shrinking a training dataset to a set of representative vectors | <ul><li>[Vladimir Iashin](https://iashin.ai/)</li> <li>[Esa Rahtu](https://esa.rahtu.fi/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/2110.08791), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1711.00937), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00820), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1712.01393), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1512.08512)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/v-iashin/SpecVQGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/PeihaoChen/regnet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/toshas/torch-fidelity), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/descriptinc/melgan-neurips), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/lyra)</li><li>[project](https://iashin.ai/SpecVQGAN)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Foley_(filmmaking)), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Row-_and_column-major_order), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Bucb3nAa398)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1pxTIMweAKApJZ3ZFqyBee3HtMqFpnwQ0) | 03.02.2022 |
| JoJoGAN | One Shot Face Stylization | <ul><li>[Min Jin Chong](https://mchong6.github.io/)</li> <li>[David Forsyth](http://luthuli.cs.uiuc.edu/~daf/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.11641)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mchong6/JoJoGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/replicate/cog)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/master/stylize.ipynb) | 02.02.2022 |
| XLS-R | Self-supervised Cross-lingual Speech Representation Learning at Scale | <ul><li>[Arun Babu](https://github.com/arbabu123)</li> <li>[Changhan Wang](https://www.changhan.me/)</li> <li>[Andros Tjandra](https://github.com/androstj)</li> <li>[Kushal Lakhotia](https://about.me/hikushalhere)</li> <li>[Qiantong Xu](https://github.com/xuqiantong)</li> <li>[Naman Goyal](https://scholar.google.com/citations?user=CRbM_P4AAAAJ)</li> <li>[Kritika Singh](https://scholar.google.com/citations?user=Ltk3SykAAAAJ)</li> <li>[Patrick von Platen](https://github.com/patrickvonplaten)</li> <li>[Yatharth Saraf](https://scholar.google.com/citations?user=KJTtNJwAAAAJ)</li> <li>[Juan Pino](https://scholar.google.com/citations?user=weU_-4IAAAAJ)</li> <li>[Alexei Baevski](https://github.com/alexeib)</li> <li>[Alexis Conneau](https://github.com/aconneau)</li> <li>[Michael Auli](https://github.com/michaelauli)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.09296)</li><li>[blog post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/xlsr/README.md), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/fairscale)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLS_R_on_Common_Voice.ipynb) | 02.02.2022 |
| DFL-Colab | This project provides you IPython Notebook to use DeepFaceLab | [chervonij](https://github.com/chervonij) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.05535)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/iperov/DeepFaceLab)</li><li>[guide](https://mrdeepfakes.com/forums/thread-guide-deepfacelab-google-colab-tutorial)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/channel/UCTKBl8kB6DJ_qLnk1NGDGbQ)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/chervonij/DFL-Colab/blob/master/DFL_Colab.ipynb) | 20.01.2022 |
| LaMa | Resolution-robust Large Mask Inpainting with Fourier Convolutions | <ul><li>[Roman Suvorov](https://github.com/windj007)</li> <li>[Elizaveta Logacheva](https://github.com/elimohl)</li> <li>[Anton Mashikhin](https://www.linkedin.com/in/heyt0ny/)</li> <li>[Anastasia Remizova](https://github.com/feathernox)</li> <li>[Arsenii Ashukha](https://ashukha.com/)</li> <li>[Aleksei Silvestrov](https://www.linkedin.com/in/%D0%B0%D0%BB%D0%B5%D0%BA%D1%81%D0%B5%D0%B9-%D1%81%D0%B8%D0%BB%D1%8C%D0%B2%D0%B5%D1%81%D1%82%D1%80%D0%BE%D0%B2-141b99b6/)</li> <li>[Naejin Kong](https://github.com/naejin-kong)</li> <li>[Harshith Goka](https://github.com/h9399-goka)</li> <li>[Kiwoong Park](https://github.com/kyoong-park)</li> <li>[Victor Lempitsky](http://sites.skoltech.ru/compvision/members/vilem/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.07161)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/saic-mdal/lama), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/andy971022/auto-lama), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/richzhang/PerceptualSimilarity), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Po-Hsun-Su/pytorch-ssim), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/mseitzer/pytorch-fid)</li><li>[project](https://saic-mdal.github.io/lama-project/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/saic-mdal/lama/blob/master/colab/LaMa_inpainting.ipynb) | 16.01.2022 |
| Taming Transformers for High-Resolution Image Synthesis | We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer | <ul><li>[Patrick Esser](https://github.com/pesser)</li> <li>[Robin Rombach](https://github.com/rromb)</li> <li>[Björn Ommer](https://hci.iwr.uni-heidelberg.de/people/bommer)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CompVis/taming-transformers)</li><li>[project](https://compvis.github.io/taming-transformers/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb) | 13.01.2022 |
| FuseDream | Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization | <ul><li>[Xingchao Liu](https://scholar.google.com/citations?user=VOTVE0UAAAAJ)</li> <li>[Chengyue Gong](https://github.com/ChengyueGongR)</li> <li>[Lemeng Wu](https://github.com/klightz)</li> <li>[Hao Su](https://cseweb.ucsd.edu//~haosu/)</li> <li>[Qiang Liu](https://www.cs.utexas.edu/~lqiang/)</li></ul> | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.01573) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/190tKQf0aFj-Hi8STUrLc2m4DeOviv7NO) | 02.01.2022 |
| HuggingArtists | Choose your favorite Artist and train a language model to write new lyrics based on their unique voice | [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [<img src="images/git.svg" alt="git" height=20/>](https://github.com/AlekseyKorshuk/huggingartists) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) | 22.12.2021 |
| Music Composer | Synthesizing symbolic music in MIDI format using the Music Transformer model | [bazanovvanya](https://github.com/bazanovvanya) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1909.05858)</li><li>[blog post](https://habr.com/ru/company/sberbank/blog/583592/)</li><li>[data](https://magenta.tensorflow.org/datasets/maestro), [data](https://colinraffel.com/projects/lmd/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sberbank-ai/music-composer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/gwinndr/MusicTransformer-Pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/bytedance/GiantMIDI-Piano), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdeff/fma)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/sberbank-ai/music-composer/blob/master/src/Music_Composer_Demo_Colab_en.ipynb) | 20.12.2021 |
| StyleGAN-NADA | Zero-Shot non-adversarial domain adaptation of pre-trained generators | <ul><li>[Rinon Gal](https://rinongal.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Haggai Maron](https://haggaim.github.io/)</li> <li>[Gal Chechik](https://research.nvidia.com/person/gal-chechik)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.17249), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.02699)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rinongal/StyleGAN-nada), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch/), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada)</li><li>[project](https://stylegan-nada.github.io/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/rinongal/stylegan-nada/blob/main/stylegan_nada.ipynb) | 04.12.2021 |
| encoder4editing | Designing an Encoder for StyleGAN Image Manipulation | <ul><li>[Omer Tov](https://github.com/omertov)</li> <li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Yotam Nitzan](https://yotamnitzan.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.02766)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/omertov/encoder4editing), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/eladrich/pixel2style2pixel)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/omertov/encoder4editing/blob/master/notebooks/inference_playground.ipynb) | 02.12.2021 |
| StyleCariGAN | Caricature Generation via StyleGAN Feature Map Modulation | <ul><li>[Wonjong Jang](https://wonjongg.github.io/)</li> <li>[Gwangjin Ju](https://github.com/jugwangjin)</li> <li>[Yucheol Jung](https://ycjung.info/)</li> <li>[Jiaolong Yang](https://jlyang.org/)</li> <li>[Xin Tong](https://www.microsoft.com/en-us/research/people/xtong/)</li> <li>[Seungyong Lee](https://scholar.google.com/citations?user=yGPH-nAAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.04331)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/wonjongg/StyleCariGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li><li>[project](https://wonjongg.github.io/StyleCariGAN/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=kpHbGOlI-BU)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1HDRQGm7pvC9mAb6Lktoft_SmY9sCq_Qg) | 30.11.2021 |
| DALL·E Mini | Generate images from a text prompt | <ul><li>[Boris Dayma](https://github.com/borisdayma)</li> <li>[Suraj Patil](https://github.com/patil-suraj)</li> <li>[Pedro Cuenca](https://github.com/pcuenca)</li> <li>[Khalid Saifullah](https://khalidsaifullaah.github.io/)</li> <li>[Tanishq Abraham](https://github.com/tmabraham)</li> <li>[Phúc H. Lê Khắc](https://lkhphuc.com/)</li> <li>[Luke Melas](https://lukemelas.github.io/)</li> <li>[Ritobrata Ghosh](https://ghosh-r.github.io/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.08981), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.13461), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1807.04015)</li><li>[blog post](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA)</li><li>[data](https://aclanthology.org/P18-1238/)</li><li>[demo](https://huggingface.co/spaces/flax-community/dalle-mini)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/borisdayma/dalle-mini), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/huggingface/transformers/tree/master/examples/research_projects/jax-projects), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP/blob/main/data/yfcc100m.md)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/borisdayma/dalle-mini/blob/main/tools/inference/inference_pipeline.ipynb) | 28.11.2021 |
| CartoonGAN | The implementation of the cartoon GAN model with PyTorch | [Tobias Sunderdiek](https://github.com/TobiasSunderdiek) | <ul><li>[cvpr](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/alamson/safebooru)</li><li>[project](https://tobiassunderdiek.github.io/cartoon-gan/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb) | 24.11.2021 |
| SimSwap | An efficient framework, called Simple Swap, aiming for generalized and high fidelity face swapping | <ul><li>[Xuanhong Chen](https://github.com/neuralchen)</li> <li>[Bingbing Ni](https://scholar.google.com.sg/citations?user=eUbmKwYAAAAJ)</li> <li>[Yanhao Ge](https://scholar.google.com/citations?user=h6tuBAcAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.06340)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/neuralchen/SimSwap), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepinsight/insightface)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/neuralchen/SimSwap/blob/master/SimSwap%20colab.ipynb) | 24.11.2021 |
| RVM | Robust High-Resolution Video Matting with Temporal Guidance | <ul><li>[Peter Lin](https://github.com/PeterL1n)</li> <li>[Linjie Yang](https://sites.google.com/site/linjieyang89/)</li> <li>[Imran Saleemi](http://www.cs.ucf.edu/~imran/)</li> <li>[Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/2108.11515)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/PeterL1n/RobustVideoMatting), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/VideoProcessingFramework), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/FeiGeChuanShu/ncnn_Android_RobustVideoMatting)</li><li>[project](https://peterl1n.github.io/RobustVideoMatting)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Jvzltozpbpk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Ay-mGCEYEzM)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/10z-pNKRnVNsp0Lq9tH1J_XPZ7CBC_uHm) | 24.11.2021 |
| AnimeGANv2 | An improved version of AnimeGAN - it prevents the generation of high-frequency artifacts by simply changing the normalization of features in the network | <ul><li>[Xin Chen](https://dl.acm.org/profile/99659508903)</li> <li>[Gang Liu](https://dl.acm.org/profile/81493643454)</li> <li>[bryandlee](https://github.com/bryandlee)</li></ul> | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/bryandlee/animegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TachibanaYoshino/AnimeGANv2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TachibanaYoshino/AnimeGAN)</li><li>[project](https://tachibanayoshino.github.io/AnimeGANv2/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/bryandlee/animegan2-pytorch/blob/master/colab_demo.ipynb) | 17.11.2021 |
| YOLOv3 | You Only Look Once | [Glenn Jocher](https://github.com/glenn-jocher) | <ul><li>[data](http://cocodataset.org/#upload)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ultralytics/yolov3)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/yolov3), [<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/coco128)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb) | 14.11.2021 |
| SOAT | StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN | <ul><li>[Min Jin Chong](https://mchong6.github.io/)</li> <li>[Hsin-Ying Lee](http://hsinyinglee.com/)</li> <li>[David Forsyth](http://luthuli.cs.uiuc.edu/~daf/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.01619)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mchong6/SOAT), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/justinpinkney/toonify), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mchong6/SOAT/blob/master/infinity.ipynb) | 13.11.2021 |
| Arnheim | Generative Art Using Neural Visual Grammars and Dual Encoders | <ul><li>[Chrisantha Fernando](https://www.chrisantha.co.uk/)</li> <li>[Ali Eslami](http://arkitus.com/)</li> <li>[Jean-Baptiste Alayrac](https://www.jbalayrac.com/)</li> <li>[Piotr Mirowski](https://piotrmirowski.com/)</li> <li>[Dylan Banarse](https://www.2ne1.com/)</li> <li>[Simon Osindero](https://scholar.google.com/citations?user=Jq8ZS5kAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.00162), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.14843), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1801.07729), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1606.02580), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1609.09106)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/arnheim), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/dall-e)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Compositional_pattern-producing_network)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=U7guaMdeF4g), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=zh0goLbS-l0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=SYJGNt7yu6M), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=MxkYKa0x5AU)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/deepmind/arnheim/blob/master/arnheim_2.ipynb) | 11.11.2021 |
| StyleGAN 2 | Generation of faces, cars, etc. | [Mikael Christensen](https://github.com/Syntopia) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.04958) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH) | 05.11.2021 |
| ruDALL-E | Generate images from texts in Russian | [Alex Shonenkov](https://github.com/shonenkov) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sberbank-ai/ru-dalle), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/bes-dev/vqvae_dwt_distiller.pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/boomb0om/Real-ESRGAN-colab)</li><li>[project](https://rudalle.ru/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/sberbank-ai/ru-dalle/blob/master/jupyters/ruDALLE-example-generation-A100.ipynb) | 03.11.2021 |
| ByteTrack | Multi-Object Tracking by Associating Every Detection Box | <ul><li>[Yifu Zhang](https://github.com/ifzhang)</li> <li>[Peize Sun](https://peizesun.github.io/)</li> <li>[Yi Jiang](https://github.com/iFighting)</li> <li>[Dongdong Yu](https://miracle-fmh.github.io/)</li> <li>[Ping Luo](http://luoping.me/)</li> <li>[Xinggang Wang](https://xinggangw.info/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2110.06864)</li><li>[data](https://motchallenge.net/), [data](https://www.crowdhuman.org/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ifzhang/ByteTrack), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Megvii-BaseDetection/YOLOX), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ifzhang/FairMOT), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/PeizeSun/TransTrack), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/samylee/Towards-Realtime-MOT-Cpp)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/multi-object-tracking)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1bDilg4cmXFa8HCKHbsZ_p16p0vrhLyu0) | 30.10.2021 |
| T0 | Multitask Prompted Training Enables Zero-Shot Task Generalization | <ul><li>[Victor Sanh](https://github.com/VictorSanh)</li> <li>[Albert Webson](https://representation.ai/)</li> <li>[Colin Raffel](https://colinraffel.com/)</li> <li>[Stephen Bach](http://cs.brown.edu/people/sbach/)</li> <li>[Lintang Sutawika](https://github.com/lintangsutawika)</li> <li>[Zaid Alyafeai](https://github.com/zaidalyafeai)</li> <li>[Antoine Chaffin](https://antoine.chaffin.fr/)</li> <li>[Arnaud Stiegler](https://github.com/arnaudstiegler)</li> <li>[Teven Le Scao](https://scholar.google.com/citations?user=ik0_vxsAAAAJ)</li> <li>[Arun Raja](https://www.arunraja.dev/)</li> <li>[Manan Dey](https://github.com/manandey)</li> <li>[M Saiful Bari](https://sbmaruf.github.io/)</li> <li>[Canwen Xu](https://www.canwenxu.net/)</li> <li>[Urmish Thakker](https://github.com/Urmish)</li> <li>[Shanya Sharma](https://shanyas10.github.io/)</li> <li>[Eliza Szczechla](https://elsanns.github.io/)</li> <li>[Taewoon Kim](https://tae898.github.io/)</li> <li>[Gunjan Chhablani](https://gchhablani.github.io/)</li> <li>[Nihal Nayak](https://nihalnayak.github.io/)</li> <li>[Debajyoti Datta](http://debajyotidatta.github.io/)</li> <li>[Jonathan Chang](https://github.com/cccntu/)</li> <li>[Mike Tian-Jian Jiang](https://github.com/tianjianjiang)</li> <li>[Matteo Manica](https://github.com/drugilsberg)</li> <li>[Sheng Shen](https://sincerass.github.io/)</li> <li>[Zheng Xin Yong](https://yongzx.github.io/)</li> <li>[Harshit Pandey](https://scholar.google.com/citations?user=BPIs78gAAAAJ)</li> <li>[Rachel Bawden](https://rbawden.github.io/)</li> <li>[Trishala Neeraj](https://github.com/trishalaneeraj)</li> <li>[Jos Rozen](https://scholar.google.com/citations?user=OxEDKogAAAAJ)</li> <li>[Abheesht Sharma](https://github.com/abheesht-sharma)</li> <li>[Andrea Santilli](https://teelinsan.github.io/)</li> <li>[Thibault Fevry](http://thibaultfevry.com/)</li> <li>[Jason Alan Fries](https://web.stanford.edu/~jfries/)</li> <li>[Ryan Teehan](https://github.com/rteehas)</li> <li>[Stella Biderman](https://www.stellabiderman.com/)</li> <li>[Leo Gao](https://github.com/leogao2)</li> <li>[Tali Bers](https://github.com/tbers-coursera)</li> <li>[Thomas Wolf](https://thomwolf.io/)</li> <li>[Alexander M. Rush](https://scholar.google.com/citations?user=LIjnUGgAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2110.08207)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/bigscience-workshop/promptsource/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/iJ0IVZgGjTM), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/YToXXfrIu6w)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1xx7SgdLaAu23YFBirXmaQViDr8caowX_) | 26.10.2021 |
| StyleGAN3 | Alias-Free Generative Adversarial Networks | <ul><li>[Tero Karras](https://research.nvidia.com/person/tero-karras)</li> <li>[Miika Aittala](https://research.nvidia.com/person/Miika-Aittala)</li> <li>[Samuli Laine](https://research.nvidia.com/person/Samuli-Laine)</li> <li>[Erik Härkönen](https://github.com/harskish)</li> <li>[Janne Hellsten](https://research.nvidia.com/person/Janne-Hellsten)</li> <li>[Jaakko Lehtinen](https://github.com/jtlehtin)</li> <li>[Timo Aila](https://research.nvidia.com/person/timo-aila)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.12423), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.08500), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1801.01401), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.06991), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1812.04948), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1606.03498)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan3), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan3-detector), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/metfaces-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada)</li><li>[project](https://nvlabs.github.io/stylegan3)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1BXNHZBai-pXtP-ncliouXo_kUiG1Pq7M) | 19.10.2021 |
| GPT-2 | Retrain an advanced text generating neural network on any text dataset using gpt-2-simple! | [Max Woolf](https://minimaxir.com/) | <ul><li>[blog post](https://minimaxir.com/2019/09/howto-gpt2/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/gpt-2-simple)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) | 18.10.2021 |
| SwinIR | Image Restoration Using Swin Transformer | <ul><li>[Jingyun Liang](https://jingyunliang.github.io/)</li> <li>[Jiezhang Cao](https://github.com/caojiezhang)</li> <li>[Guolei Sun](https://github.com/GuoleiSun)</li> <li>[Kai Zhang](https://cszn.github.io/)</li> <li>[Luc Van Gool](https://scholar.google.com/citations?user=TwMib_QAAAAJ)</li> <li>[Radu Timofte](http://people.ee.ethz.ch/~timofter/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.10257), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.10833)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/JingyunLiang/SwinIR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/cszn/BSRGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/Swin-Transformer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/cszn/KAIR)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb) | 08.10.2021 |
| IC-GAN | Instance-Conditioned GAN | <ul><li>[Arantxa Casanova](https://github.com/ArantxaCasanova)</li> <li>[Marlène Careil](https://www.linkedin.com/in/marl%C3%A8ne-careil-901804155)</li> <li>[Jakob Verbeek](http://thoth.inrialpes.fr/~verbeek/)</li> <li>[Michał Drożdżal](https://scholar.google.com/citations?user=XK_ktwQAAAAJ)</li> <li>[Adriana Romero-Soriano](https://sites.google.com/site/adriromsor)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.05070)</li><li>[blog post](https://ai.facebook.com/blog/instance-conditioned-gans/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/ic_gan), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/faiss), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ajbrock/BigGAN-PyTorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/bioinf-jku/TTUR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/mit-han-lab/data-efficient-gans)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/facebookresearch/ic_gan/blob/master/inference/icgan_colab.ipynb) | 01.10.2021 |
| Skillful Precipitation Nowcasting Using Deep Generative Models of Radar | Open-sourced dataset and model snapshot for precipitation nowcasting | <ul><li>[Suman Ravuri](https://www.linkedin.com/in/suman-ravuri-81928082)</li> <li>[Karel Lenc](https://www.robots.ox.ac.uk/~karel/)</li> <li>[Matthew Willson](https://www.linkedin.com/in/matthew-willson-6a1b422)</li> <li>[Dmitry Kangin](https://scholar.google.com/citations?user=vv-leaMAAAAJ)</li> <li>[Rémi Lam](https://scholar.google.com/citations?user=Sm7xCbEAAAAJ)</li> <li>[Piotr Mirowski](https://piotrmirowski.com/)</li> <li>[Maria Athanassiadou](https://scholar.google.com/citations?user=VtkgHP0AAAAJ)</li> <li>[Sheleem Kashem](https://www.linkedin.com/in/sheleemkashem/)</li> <li>[Rachel Prudden](https://scholar.google.com/citations?user=uf39AF8AAAAJ)</li> <li>[Amol Mandhane](https://github.com/amol-mandhane)</li> <li>[Aidan Clark](https://scholar.google.com/citations?user=uf39AF8AAAAJ)</li> <li>[Andrew Brock](https://github.com/ajbrock)</li> <li>[Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ)</li> <li>[Raia Hadsell](https://github.com/raiah)</li> <li>[Niall Robinson](https://github.com/niallrobinson)</li> <li>[Ellen Clancy](https://www.linkedin.com/in/ellen-clancy-815967124)</li> <li>[Shakir Mohamed](https://www.shakirm.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.00954)</li><li>[blog post](https://deepmind.com/blog/article/nowcasting)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/deepmind-research/tree/master/nowcasting)</li><li>[local kernel](https://research.google.com/colaboratory/local-runtimes.html)</li><li>[paper](https://www.nature.com/articles/s41586-021-03854-z)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/hub)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/deepmind/deepmind-research/blob/master/nowcasting/Open_sourced_dataset_and_model_snapshot_for_precipitation_nowcasting.ipynb) | 29.09.2021 |
| Text2Animation | Generate images from text phrases with VQGAN and CLIP with animation and keyframes | <ul><li>[Katherine Crowson](https://kath.io/)</li> <li>[Ryan Murdock](https://twitter.com/advadnoun)</li> <li>[Chigozie Nri](https://github.com/chigozienri)</li> <li>[Denis Malimonov](https://github.com/tg-bomze)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/chigozienri/VQGAN-CLIP-animations)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/channel/UCToztRy9FSTIhEen_1x4FAw)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/Text2Animation.ipynb) | 29.09.2021 |
| Live Speech Portraits | Real-Time Photorealistic Talking-Head Animation | <ul><li>[Yuanxun Lu](https://github.com/YuanxunLu)</li> <li>[Jinxiang Chai](https://scholar.google.com/citations?user=OcN1_gwAAAAJ)</li> <li>[Xun Cao](https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.10595)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/YuanxunLu/LiveSpeechPortraits), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lelechen63/ATVGnet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/DinoMan/speech-driven-animation), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)</li><li>[project](https://yuanxunlu.github.io/projects/LiveSpeechPortraits/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1tKvi-9kY3GkEK8lgtfTSM70rMFo_TY50) | 26.09.2021 |
| Open-Unmix | A deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists | <ul><li>[Fabian-Robert Stöter](http://faroit.com/)</li> <li>[Antoine Liutkus](https://github.com/aliutkus)</li></ul> | <ul><li>[data](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sigsep/open-unmix-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/sigsep/norbert)</li><li>[paper](https://www.theoj.org/joss-papers/joss.01667/10.21105.joss.01667.pdf)</li><li>[project](https://sigsep.github.io/open-unmix/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ) | 23.07.2021 |
| textgenrnn | Generate text using a pretrained neural network with a few lines of code, or easily train your own text-generating neural network of any size and complexity | [Max Woolf](https://minimaxir.com/) | <ul><li>[blog post](http://minimaxir.com/2018/05/text-neural-networks/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/textgenrnn)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=RW7mP6BfZuY)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK) | 13.07.2021 |
| Anycost GAN | Interactive natural image editing | <ul><li>[Ji Lin](http://linji.me/)</li> <li>[Richard Zhang](https://richzhang.github.io/)</li> <li>[Frieder Ganz](https://scholar.google.com/citations?user=u9ySZkUAAAAJ)</li> <li>[Song Han](https://songhan.mit.edu/)</li> <li>[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.03243)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mit-han-lab/anycost-gan), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/switchablenorms/CelebAMask-HQ), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/fyu/lsun)</li><li>[project](https://hanlab.mit.edu/projects/anycost-gan/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=_yEziPl9AkM)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mit-han-lab/anycost-gan/blob/master/notebooks/intro_colab.ipynb) | 07.07.2021 |
| First Order Motion Model for Image Animation | Transferring facial movements from video to image | [Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/first-order-model)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2019/hash/31c0b36aef265d9221af80872ceb62f9-Abstract.html)</li><li>[project](https://aliaksandrsiarohin.github.io/first-order-model-website/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=u-0cQ-grXBQ)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb) | 30.06.2021 |
| TediGAN | Framework for multi-modal image generation and manipulation with textual descriptions | <ul><li>[Weihao Xia](https://github.com/weihaox)</li> <li>[Yujiu Yang](http://www.fiesta.tsinghua.edu.cn/pi/3/24)</li> <li>[Jing-Hao Xue](http://www.homepages.ucl.ac.uk/~ucakjxu/)</li> <li>[Baoyuan Wu](https://sites.google.com/site/baoyuanwu2015/home)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.03308), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.08910)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/IIGROUP/TediGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/weihaox/Multi-Modal-CelebA-HQ), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch/), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/fyu/lsun)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/L8Na2f5viAM)</li></ul> | [![Open In Colab](images/colab.svg)](http://colab.research.google.com/github/weihaox/TediGAN/blob/master/playground.ipynb) | 30.06.2021 |
| GANs N' Roses | Stable, Controllable, Diverse Image to Image Translation | <ul><li>[Min Jin Chong](https://mchong6.github.io/)</li> <li>[David Forsyth](http://luthuli.cs.uiuc.edu/~daf/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.06561), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2007.06600)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mchong6/GANsNRoses), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/znxlwm/UGATIT-pytorch)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/VNg0NyCGl_4)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mchong6/GANsNRoses/blob/master/inference_colab.ipynb) | 19.06.2021 |
| Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes | A method to stylize images by optimizing parameterized brushstrokes instead of pixels | <ul><li>[Dmytro Kotovenko](https://scholar.google.de/citations?user=T_U8yxwAAAAJ)</li> <li>[Matthias Wright](https://matthias-wright.github.io/)</li> <li>[Arthur Heimbrecht](https://github.com/arwehei)</li> <li>[Björn Ommer](https://hci.iwr.uni-heidelberg.de/people/bommer)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.17185)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CompVis/brushstroke-parameterized-style-transfer)</li><li>[project](https://compvis.github.io/brushstroke-parameterized-style-transfer/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/CompVis/brushstroke-parameterized-style-transfer/blob/tensorflow_v2/notebooks/BrushstrokeStyleTransfer_TF2.ipynb) | 02.06.2021 |
| Pixel2Style2Pixel | Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation | <ul><li>[Elad Richardson](https://github.com/eladrich)</li> <li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Yotam Nitzan](https://yotamnitzan.github.io/)</li> <li>[Daniel Cohen-Or](https://www.cs.tau.ac.il/~dcor/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00951)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/eladrich/pixel2style2pixel), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/HuangYG123/CurricularFace)</li><li>[project](https://eladrich.github.io/pixel2style2pixel/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb) | 01.06.2021 |
| Fine-tuning a BERT | We will work through fine-tuning a BERT model using the tensorflow-models PIP package | <ul><li>[Chen Chen](https://github.com/chenGitHuber)</li> <li>[Claire Yao](https://github.com/claireyao-fen)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://tensorflow.org/hub)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb) | 24.05.2021 |
| ReStyle | A Residual-Based StyleGAN Encoder via Iterative Refinement | <ul><li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.02699), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00951), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.02766)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/yuval-alaluf/restyle-encoder), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TreB1eN/InsightFace_Pytorch)</li><li>[project](https://yuval-alaluf.github.io/restyle-encoder/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/yuval-alaluf/restyle-encoder/blob/master/notebooks/inference_playground.ipynb) | 21.05.2021 |
| Motion Representations for Articulated Animation | Novel motion representations for animating articulated objects consisting of distinct parts | <ul><li>[Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/)</li> <li>[Oliver Woodford](https://ojwoodford.github.io/)</li> <li>[Jian Ren](https://alanspike.github.io/)</li> <li>[Menglei Chai](https://mlchai.com/)</li> <li>[Sergey Tulyakov](http://www.stulyakov.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.11280)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/snap-research/articulated-animation)</li><li>[project](https://snap-research.github.io/articulated-animation/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=gpBYN8t8_yY)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/articulated-animation/blob/master/demo.ipynb) | 29.04.2021 |
| SAM | Age Transformation Using a Style-Based Regression Model | <ul><li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.02754)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/yuval-alaluf/SAM), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/eladrich/pixel2style2pixel), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li><li>[project](https://yuval-alaluf.github.io/SAM/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/X_pYC_LtBFw)</li></ul> | [![Open In Colab](images/colab.svg)](http://colab.research.google.com/github/yuval-alaluf/SAM/blob/master/notebooks/animation_inference_playground.ipynb) | 26.04.2021 |
| SkinDeep | Remove Body Tattoo Using Deep Learning | [Vijish Madhavan](https://github.com/vijishmadhavan) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02921), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1603.08155)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vijishmadhavan/SkinDeep), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jantic/DeOldify)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep_good.ipynb) | 24.04.2021 |
| Geometry-Free View Synthesis | Is a geometric model required to synthesize novel views from a single image? | <ul><li>[Robin Rombach](https://github.com/rromb)</li> <li>[Patrick Esser](https://github.com/pesser)</li> <li>[Björn Ommer](https://hci.iwr.uni-heidelberg.de/people/bommer)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.07652)</li><li>[data](https://google.github.io/realestate10k/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CompVis/geometry-free-view-synthesis), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/colmap/colmap)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/CompVis/geometry-free-view-synthesis/blob/master/scripts/braindance.ipynb) | 22.04.2021 |
| NeRViS | An algorithm for full-frame video stabilization by first estimating dense warp fields | <ul><li>[Yu-Lun Liu](http://www.cmlab.csie.ntu.edu.tw/~yulunliu/)</li> <li>[Wei-Sheng Lai](https://www.wslai.net/)</li> <li>[Ming-Hsuan Yang](https://faculty.ucmerced.edu/mhyang/)</li> <li>[Yung-Yu Chuang](https://www.csie.ntu.edu.tw/~cyy/)</li> <li>[Jia-Bin Huang](https://filebox.ece.vt.edu/~jbhuang/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.06205)</li><li>[data](http://liushuaicheng.org/SIGGRAPH2013/database.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/alex04072000/NeRViS), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/cxjyxxme/deep-online-video-stabilization), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jinsc37/DIFRINT)</li><li>[project](https://alex04072000.github.io/NeRViS/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/KO3sULs4hso)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1l-fUzyM38KJMZyKMBWw_vu7ZUyDwgdYH) | 11.04.2021 |
| NeX | View synthesis based on enhancements of multiplane image that can reproduce NeXt-level view-dependent effects in real time | <ul><li>[Suttisak Wizadwongsa](https://www.linkedin.com/in/suttisak-wizadwongsa-763a931a5/)</li> <li>[Pakkapon Phongthawee](http://pureexe.github.io/)</li> <li>[Jiraphon Yenphraphai](https://www.linkedin.com/in/jiraphon-yenphraphai-990ba6175/)</li> <li>[Supasorn Suwajanakorn](https://www.supasorn.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.05606)</li><li>[data](https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fshiny%5Fdatasets&originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VuSVVoc1JWSk9kTnNaXzRzbWRoeWUwQjh6MFZseHFPUjM1SVIzYnAwdUd1cFE%5FcnRpbWU9WXRVQTQtQTcyVWc), [data](https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VyalBSUkw5Sm5GSXA4TU42ZDFqRXVvQjNYVm94SmtmZlBqZm9QeWhIa2owZGc%5FcnRpbWU9bC0yYWctRTcyVWc&id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fmodified%5Fdataset)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/nex-mpi/nex-code), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Fyusion/LLFF)</li><li>[project](https://nex-mpi.github.io/)</li><li>[vistec](https://vistec.ist/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=HyfkF7Z-ddA)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1hXVvYdAwLA0EFg2zrafJUE0bFgB_F7PU) | 25.03.2021 |
| Big Sleep | Text to image generation, using OpenAI's CLIP and a BigGAN | [Phil Wang](https://github.com/lucidrains) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/lucidrains/big-sleep), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/bigsleep/comments/lxawb4/how_to_use_some_of_the_newer_features_of/), [<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/bigsleep/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb) | 17.03.2021 |
| Deep Daze | Text to image generation using OpenAI's CLIP and Siren | [Phil Wang](https://github.com/lucidrains) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.09661)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/lucidrains/deep-daze), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/deepdaze/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj) | 17.03.2021 |
| Talking Head Anime from a Single Image | The network takes as input an image of an anime character's face and a desired pose, and it outputs another image of the same character in the given pose | [Pramook Khungurn](https://pkhungurn.github.io/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/pkhungurn/talking-head-anime-demo), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lincolnhard/head-pose-estimation)</li><li>[project](https://pkhungurn.github.io/talking-head-anime/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Virtual_YouTuber), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/MikuMikuDance)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/kMQCERkTdO0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/T1Gp-RxFZwU), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/FioRJ6x_RbI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/pkhungurn/talking-head-anime-demo/blob/master/tha_colab.ipynb) | 23.02.2021 |
| Multitrack MusicVAE | The models in this notebook are capable of encoding and decoding single measures of up to 8 tracks, optionally conditioned on an underlying chord | <ul><li>[Ian Simon](https://github.com/iansimon)</li> <li>[Adam Roberts](https://github.com/adarob)</li> <li>[Colin Raffel](https://colinraffel.com/)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Douglas Eck](https://github.com/douglaseck)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1806.00195)</li><li>[blog post](http://g.co/magenta/multitrack)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) | 17.02.2021 |
| NFNet | An adaptive gradient clipping technique, a significantly improved class of Normalizer-Free ResNets | <ul><li>[Andrew Brock](https://github.com/ajbrock)</li> <li>[Soham De](https://sohamde.github.io/)</li> <li>[Samuel L. Smith](https://scholar.google.co.uk/citations?user=fyEqU5oAAAAJ)</li> <li>[Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.06171), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2101.08692)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/deepmind-research/tree/master/nfnets), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/jaxline)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/deepmind/deepmind-research/blob/master/nfnets/nfnet_demo_colab.ipynb) | 17.02.2021 |
| bsuite | A collection of carefully-designed experiments that investigate core capabilities of an RL agent with two main objectives | [Ian Osband](http://iosband.github.io/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/bsuite), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/gym)</li><li>[paper](https://openreview.net/forum?id=rygf-kSYwH)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1rU20zJ281sZuMD1DHbsODFr1DbASL0RH) | 13.02.2021 |
| Wav2Lip | A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild | <ul><li>[Prajwal Renukanand](https://github.com/prajwalkr)</li> <li>[Rudrabha Mukhopadhyay](https://rudrabha.github.io/)</li> <li>[Vinay Namboodiri](https://vinaypn.github.io/)</li> <li>[C. V. Jawahar](https://faculty.iiit.ac.in/~jawahar/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.10010)</li><li>[data](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)</li><li>[demo](http://bhaasha.iiit.ac.in/lipsync/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Rudrabha/Wav2Lip)</li><li>[project](http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=0fXaDCZNOJc)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/eyaler/avatars4all/blob/master/melaflefon.ipynb) | 12.02.2021 |
| CLIP | A neural network which efficiently learns visual concepts from natural language supervision | <ul><li>[Jong Wook](https://jongwook.kim/)</li> <li>[Alec Radford](http://newmu.github.io/)</li> <li>[Ilya Sutskever](http://www.cs.utoronto.ca/~ilya/)</li></ul> | <ul><li>[data](https://www.cs.toronto.edu/~kriz/cifar.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP)</li><li>[paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)</li><li>[project](https://openai.com/blog/clip/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb) | 29.01.2021 |
| Adversarial Patch | A method to create universal, robust, targeted adversarial image patches in the real world | [Tom Brown](https://github.com/nottombrown) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1712.09665) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/cleverhans-lab/cleverhans/blob/master/examples/adversarial_patch/AdversarialPatch.ipynb) | 27.01.2021 |
| MSG-Net | Multi-style Generative Network with a novel Inspiration Layer, which retains the functionality of optimization-based approaches and has the fast speed of feed-forward networks | <ul><li>[Hang Zhang](https://hangzhang.org/)</li> <li>[Kristin Dana](https://www.ece.rutgers.edu/~kdana/dana.html)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06953)</li><li>[project](http://computervisionrutgers.github.io/MSG-Net/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=oy6pWNWBt4Y)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/zhanghang1989/PyTorch-Multi-Style-Transfer/blob/master/msgnet.ipynb) | 25.01.2021 |
| Toon-Me | A fun project to toon portrait images | [Vijish Madhavan](https://github.com/vijishmadhavan) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02921), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1603.08155)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vijishmadhavan/Toon-Me)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/vijishmadhavan/Light-Up/blob/master/Toon_Me_(Try_it_on_Colab).ipynb) | 22.01.2021 |
| Neural Style Transfer | Implementation of Neural Style Transfer in Keras 2.0+ | [Somshubra Majumdar](http://titu1994.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1508.06576), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1605.04603), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1606.05897)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/titu1994/Neural-Style-Transfer)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/titu1994/Neural-Style-Transfer/blob/master/NeuralStyleTransfer.ipynb) | 22.01.2021 |
| SkyAR | A vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles | [Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.11800)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jiupinjia/SkyAR)</li><li>[project](https://jiupinjia.github.io/skyar/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=zal9Ues0aOQ)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/jiupinjia/SkyAR/blob/master/colab_demo.ipynb) | 18.01.2021 |
| Big GAN | Large Scale GAN Training for High Fidelity Natural Image Synthesis | [Google](https://www.tensorflow.org/hub) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) | 12.01.2021 |
| GrooVAE | Some applications of machine learning for generating and manipulating beats and drum performances | <ul><li>[Jon Gillick](https://www.jongillick.com/)</li> <li>[Adam Roberts](https://github.com/adarob)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.06118)</li><li>[blog post](https://g.co/magenta/groovae)</li><li>[data](https://g.co/magenta/groove-datasets)</li><li>[<img src="images/git.svg" alt="git" height=20/>](http://goo.gl/magenta/musicvae-py)</li><li>[web app](https://groove-drums.glitch.me/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=x2YLmXzovDo)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/magenta-demos/blob/master/colab-notebooks/GrooVAE.ipynb) | 08.01.2021 |
| MusicXML Documentation | The goal of this notebook is to explore one of the magenta libraries for music | <ul><li>[Prakruti Joshi](https://github.com/prakruti-joshi)</li> <li>[Falak Shah](https://falaktheoptimist.github.io/)</li> <li>[Twisha Naik](https://github.com/twisha96)</li></ul> | <ul><li>[magenta](https://magenta.tensorflow.org/)</li><li>[music theory](http://musictheoryblog.blogspot.com/2008/02/learn-music-theory.html)</li><li>[musicXML](https://www.musicxml.com/for-developers/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicXML_Document_Structure_Documentation.ipynb) | 08.01.2021 |
| SVG VAE | A colab demo for the SVG VAE model | [Raphael Gontijo Lopes](https://raphagl.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.02632)</li><li>[blog post](https://magenta.tensorflow.org/svg-vae)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/vae_svg_decoding.ipynb) | 08.01.2021 |
| PIFuHD | Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization | <ul><li>[Shunsuke Saito](https://github.com/shunsukesaito)</li> <li>[Tomas Simon](http://www.cs.cmu.edu/~tsimon/)</li> <li>[Jason Saragih](https://scholar.google.com/citations?user=ss-IvjMAAAAJ)</li> <li>[Hanbyul Joo](https://jhugestar.github.io/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.00452)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/pifuhd)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/uEDqCxvF5yc), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=8qnwbbDS8xk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt) | 05.01.2021 |
| Neural Magic Eye | Learning to See and Understand the Scene Behind an Autostereogram | <ul><li>[Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/)</li> <li>[Tianyang Shi](https://www.shitianyang.tech/)</li> <li>[Yi Yuan](https://yiyuan1991.github.io/)</li> <li>[Zhenwei Shi](http://levir.buaa.edu.cn/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.15692)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jiupinjia/neural-magic-eye)</li><li>[project](https://jiupinjia.github.io/neuralmagiceye/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Fkh7DEblqJ8)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1f59dFLJ748i2TleE54RkbUZSMo9Hyx7l) | 01.01.2021 |
| Flow-edge Guided Video Completion | Method first extracts and completes motion edges, and then uses them to guide piecewise-smooth flow completion with sharp edges | <ul><li>[Chen Gao](http://chengao.vision/)</li> <li>[Ayush Saraf](https://github.com/ayush29feb)</li> <li>[Johannes Kopf](https://johanneskopf.de/)</li> <li>[Jia-Bin Huang](https://filebox.ece.vt.edu/~jbhuang/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2009.01835)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vt-vl-lab/FGVC)</li><li>[project](http://chengao.vision/FGVC/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=CHHVPxHT7rc)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1pb6FjWdwq_q445rG2NP0dubw7LKNUkqc) | 30.12.2020 |
| ArtLine | A Deep Learning based project for creating line art portraits | [Vijish Madhavan](https://github.com/vijishmadhavan) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02921), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1603.08155)</li><li>[data](https://cg.cs.tsinghua.edu.cn/people/~Yongjin/APDrawingDB.zip)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vijishmadhavan/ArtLine), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/yiranran/APDrawingGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jantic/DeOldify)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/vijishmadhavan/Light-Up/blob/master/ArtLine(AR).ipynb) | 24.12.2020 |
| WikiArt (stylegan2-ada) | Generation of paintings of different styles and genres | [Doron Adler](https://linktr.ee/Norod78) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.06676) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/Norod/my-colab-experiments/blob/master/WikiArt_ADA_Example_Generation.ipynb) | 08.12.2020 |
| GANSpace | A simple technique to analyze GANs and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day | <ul><li>[Erik Härkönen](https://github.com/harskish)</li> <li>[Aaron Hertzmann](http://www.dgp.toronto.edu/~hertzman/)</li> <li>[Jaakko Lehtinen](https://users.aalto.fi/~lehtinj7/)</li> <li>[Sylvain Paris](http://people.csail.mit.edu/sparis/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.02546)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/harskish/ganspace), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/justinpinkney/awesome-pretrained-stylegan), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/CSAILVision/GANDissect)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/jdTICDa_eAI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/harskish/ganspace/blob/master/notebooks/Ganspace_colab.ipynb) | 06.12.2020 |
| SeFa | A closed-form approach for unsupervised latent semantic factorization in GANs | <ul><li>[Yujun Shen](https://shenyujun.github.io/)</li> <li>[Bolei Zhou](http://bzhou.ie.cuhk.edu.hk/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2007.06600)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/genforce/sefa)</li><li>[project](https://genforce.github.io/sefa/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=OFHW2WbXXIQ)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/genforce/sefa/blob/master/docs/SeFa.ipynb) | 06.12.2020 |
| Stylized Neural Painting | An image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles | <ul><li>[Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/)</li> <li>[Tianyang Shi](https://www.shitianyang.tech/)</li> <li>[Yi Yuan](https://yiyuan1991.github.io/)</li> <li>[Zhenwei Shi](http://levir.buaa.edu.cn/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.08114)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jiupinjia/stylized-neural-painting)</li><li>[project](https://jiupinjia.github.io/neuralpainter/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=oerb-nwrXhk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1ch_41GtcQNQT1NLOA21vQJ_rQOjjv9D8) | 01.12.2020 |
| DeOldify (video) | Colorize your own videos! | [Jason Antic](https://github.com/jantic) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.08500)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42)</li><li>[model](https://data.deepai.org/deoldify/ColorizeVideo_gen.pth)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/Nickelodeons/), [<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/silentmoviegifs/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](http://www.youtube.com/watch?v=l3UXXid04Ys), [<img src="images/youtube.svg" alt="youtube" height=20/>](http://www.youtube.com/watch?v=EXn-n2iqEjI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb) | 13.11.2020 |
| DeOldify (photo) | Colorize your own photos! | <ul><li>[Jason Antic](https://github.com/jantic)</li> <li>[Matt Robinson](https://github.com/mc-robinson)</li> <li>[María Benavente](https://github.com/mariabg)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.08500)</li><li>[model](https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/TheWayWeWere/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb) | 13.11.2020 |
| MakeItTalk | A method that generates expressive talking-head videos from a single facial image with audio as the only input | <ul><li>[Yang Zhou](https://people.umass.edu/~yangzhou/)</li> <li>[Xintong Han](http://users.umiacs.umd.edu/~xintong/)</li> <li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li> <li>[Jose Echevarria](http://www.jiechevarria.com/)</li> <li>[Evangelos Kalogerakis](https://people.cs.umass.edu/~kalo/)</li> <li>[Dingzeyu Li](https://dingzeyu.li/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.12992)</li><li>[data](https://drive.google.com/drive/folders/1EwuAy3j1b9Zc1MsidUfxG_pJGc_cV60O)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/yzhou359/MakeItTalk)</li><li>[project](https://people.umass.edu/~yangzhou/MakeItTalk/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=vUMGKASgbf8)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/yzhou359/MakeItTalk/blob/master/quick_demo.ipynb) | 10.11.2020 |
| LaSAFT | Latent Source Attentive Frequency Transformation for Conditioned Source Separation | [Woosung Choi](https://ws-choi.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.11631)</li><li>[data](https://sigsep.github.io/datasets/musdb.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ws-choi/Conditioned-Source-Separation-LaSAFT)</li><li>[project](https://lasaft.github.io/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/ws-choi/Conditioned-Source-Separation-LaSAFT/blob/master/colab_demo/LaSAFT_with_GPoCM_Stella_Jang_Example.ipynb) | 01.11.2020 |
| Lifespan Age Transformation Synthesis | Multi-domain image-to-image generative adversarial network architecture, whose learned latent space models a continuous bi-directional aging process | <ul><li>[Roy Or-El](https://homes.cs.washington.edu/~royorel/)</li> <li>[Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/)</li> <li>[Ohad Fried](https://www.ohadf.com/)</li> <li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li> <li>[Ira Kemelmacher-Shlizerman](https://www.irakemelmacher.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2003.09764)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/royorel/Lifespan_Age_Transformation_Synthesis), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/royorel/FFHQ-Aging-Dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/pix2pixHD), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/style-based-gan-pytorch)</li><li>[project](https://grail.cs.washington.edu/projects/lifespan_age_transformation_synthesis/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/_jTFcjN2hBk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/9fulnt2_q_Y)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/royorel/Lifespan_Age_Transformation_Synthesis/blob/master/LATS_demo.ipynb) | 31.10.2020 |
| HiGAN | Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis | <ul><li>[Ceyuan Yang](https://ceyuan.me/)</li> <li>[Yujun Shen](https://shenyujun.github.io/)</li> <li>[Bolei Zhou](http://bzhou.ie.cuhk.edu.hk/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1911.09267), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1412.6856), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1906.10112)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/genforce/higanc)</li><li>[project](https://genforce.github.io/higan/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=X5yWu2Jwjpg)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/genforce/higan/blob/master/docs/HiGAN_Bedroom.ipynb) | 14.10.2020 |
| InterFaceGAN | Interpreting the Latent Space of GANs for Semantic Face Editing | <ul><li>[Yujun Shen](https://shenyujun.github.io/)</li> <li>[Jinjin Gu](https://www.jasongt.com/)</li> <li>[Xiaoou Tang](https://www.ie.cuhk.edu.hk/people/xotang.shtml)</li> <li>[Bolei Zhou](http://bzhou.ie.cuhk.edu.hk/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.10786), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.09635), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/genforce/interfacegan), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tkarras/progressive_growing_of_gans), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan)</li><li>[project](https://genforce.github.io/interfacegan/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=uoftpl3Bj6w)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/genforce/interfacegan/blob/master/docs/InterFaceGAN.ipynb) | 13.10.2020 |
| Faceswap-GAN | A minimum demo for faceswap-GAN v2.2 | [shaoanlu](https://shaoanlu.github.io/) | [<img src="images/git.svg" alt="git" height=20/>](https://github.com/shaoanlu/faceswap-GAN) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/faceswap-GAN/blob/master/colab_demo/faceswap-GAN_colab_demo.ipynb) | 12.09.2020 |
| Instance-aware Image Colorization | Novel deep learning framework to achieve instance-aware colorization | [Jheng-Wei Su](https://github.com/ericsujw) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.10825)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ericsujw/InstColorization)</li><li>[project](https://ericsujw.github.io/InstColorization/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Zj1N4uE1ehk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/ericsujw/InstColorization/blob/master/InstColorization.ipynb) | 30.08.2020 |
| Person Remover | Project that combines Pix2Pix and YOLO arhitectures in order to remove people or other objects from photos | <ul><li>[Javier Gamazo](https://www.javiergamazo.com/)</li> <li>[Daryl Autar](https://github.com/Daryl149)</li></ul> | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/javirk/Person_remover), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/javirk/Person-remover-partial-convolutions), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/zzh8829/yolov3-tf2)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=_dRjY9gMcxE)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1JDpH8MAjaKoekQ_H9ZaxYJ9_axiDtDGm) | 22.08.2020 |
| Rewriting a Deep Generative Model | We ask if a deep network can be reprogrammed to follow different rules, by enabling a user to directly change the weights, instead of training with a data set | <ul><li>[David Bau](https://people.csail.mit.edu/davidbau/home/)</li> <li>[Steven Liu](http://people.csail.mit.edu/stevenliu/)</li> <li>[Tongzhou Wang](https://ssnl.github.io/)</li> <li>[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)</li> <li>[Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2007.15646), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.04958)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/davidbau/rewriting), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li><li>[project](https://rewriting.csail.mit.edu/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=i2_-zNqtEPk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://rewriting.csail.mit.edu/video/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/davidbau/rewriting/blob/master/notebooks/rewriting-interface.ipynb) | 31.07.2020 |
| BERT score | An automatic evaluation metric for text generation | [Tianyi Zhang](https://tiiiger.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.09675)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Tiiiger/bert_score)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1kpL8Y_AnUUiCxFjhxSrxCsc6-sDMNb_Q) | 17.07.2020 |
| HiDT | A generative image-to-image model and a new upsampling scheme that allows to apply image translation at high resolution | <ul><li>[Denis Korzhenkov](https://github.com/denkorzh)</li> <li>[Gleb Sterkin](https://github.com/belkakari)</li> <li>[Sergey Nikolenko](https://logic.pdmi.ras.ru/~sergey/)</li> <li>[Victor Lempitsky](http://sites.skoltech.ru/compvision/members/vilem/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2003.08791)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/saic-mdal/HiDT)</li><li>[project](https://saic-mdal.github.io/HiDT/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLuvGzlEQXT1KQuKrfBBEWh2f3PToxyeM5), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=EWKAgwgqXB4)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/saic-mdal/hidt/blob/master/notebooks/HighResolutionDaytimeTranslation.ipynb) | 17.07.2020 |
| Analyzing Tennis Serve | We'll use the Video Intelligence API to analyze a tennis serve, including the angle of the arms and legs during the serve | [Dale Markowitz](https://daleonai.com/) | <ul><li>[blog post](https://daleonai.com/machine-learning-for-sports)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/making_with_ml/tree/master/sports_ai)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://manivannan-ai.medium.com/find-the-angle-between-three-points-from-2d-using-python-348c513e2cd)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=yLrOy2Xedgk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/google/making_with_ml/blob/master/sports_ai/Sports_AI_Analysis.ipynb) | 14.07.2020 |
| SIREN | Implicit Neural Representations with Periodic Activation Functions | <ul><li>[Vincent Sitzmann](https://vsitzmann.github.io/)</li> <li>[Julien Martel](http://web.stanford.edu/~jnmartel/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.09661)</li><li>[data](https://drive.google.com/drive/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vsitzmann/siren)</li><li>[project](https://vsitzmann.github.io/siren/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Q2fLWGBeaiI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb) | 24.06.2020 |
| PIFu | Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization | <ul><li>[Ryota Natsume](https://github.com/nanopoteto)</li> <li>[Shunsuke Saito](https://github.com/shunsukesaito)</li> <li>[Zeng Huang](https://zeng.science/)</li> <li>[Angjoo Kanazawa](https://people.eecs.berkeley.edu/~kanazawa/)</li> <li>[Hao Li](http://hao.li)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.05172)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/shunsukesaito/PIFu)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=S1FpjwKqtPs)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1GFSsqP2BWz4gtq0e-nki00ZHSirXwFyY) | 18.06.2020 |
| 3D Ken Burns | A reference implementation of 3D Ken Burns Effect from a Single Image using PyTorch - given a single input image, it animates this still image with a virtual camera scan and zoom subject to motion parallax | [Manuel Romero](https://mrm8488.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1909.05483)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sniklaus/3d-ken-burns)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=WrajxHHfRBA)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/3D_Ken_Burns.ipynb) | 13.06.2020 |
| MusicVAE | A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music | <ul><li>[Adam Roberts](https://github.com/adarob)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Colin Raffel](https://colinraffel.com/)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Douglas Eck](https://github.com/douglaseck)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1803.05428)</li><li>[blog post](https://g.co/magenta/music-vae)</li><li>[project](https://magenta.tensorflow.org/music-vae)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLBUMAYA6kvGU8Cgqh709o5SUvo-zHGTxr)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb) | 02.06.2020 |
| Background Matting | The notebook is split into three parts: required setup, running the algorithm on photos, and running it on videos | [Andrey Ryabtsev](https://github.com/andreyryabtsev) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.00626)</li><li>[blog post](https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635)</li><li>[data](https://drive.google.com/open?id=1j3BMrRFhFpfzJAe6P2WDtfanoeSCLPiq)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/senguptaumd/Background-Matting)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/gist/andreyryabtsev/243aa3eefa6e06891dda7b1583d1d08f/backmatting.ipynb) | 18.05.2020 |
| Jukebox | A neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles | [Christine Payne](http://christinemcleavey.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.00341)</li><li>[blog post](https://openai.com/blog/jukebox)</li><li>[explorer](http://jukebox.openai.com/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/jukebox)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb) | 04.05.2020 |
| 3D Photo Inpainting | Method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view | <ul><li>[Meng-Li Shih](https://shihmengli.github.io/)</li> <li>[Shih-Yang Su](https://lemonatsu.github.io/)</li> <li>[Johannes Kopf](https://johanneskopf.de/)</li> <li>[Jia-Bin Huang](https://filebox.ece.vt.edu/~jbhuang/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.04727)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vt-vl-lab/3d-photo-inpainting)</li><li>[project](https://shihmengli.github.io/3D-Photo-Inpainting/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz) | 04.05.2020 |
| Motion Supervised co-part Segmentation | A self-supervised deep learning method for co-part segmentation | <ul><li>[Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/)</li> <li>[Subhankar Roy](https://github.com/roysubhankar)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/2004.03234)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/motion-cosegmentation), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/video-preprocessing)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=RJ4Nj1wV5iA)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/motion-cosegmentation/blob/master/part_swap.ipynb) | 07.04.2020 |
| RNN for Predictive Maintenance | LSTM network in order to predict remaining useful life of aircraft engines | [Umberto Griffo](https://github.com/umbertogriffo) | <ul><li>[data](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb)</li><li>[link](https://gallery.azure.ai/Experiment/Predictive-Maintenance-Step-2A-of-3-train-and-evaluate-regression-models-2)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1tjIOud2Cc6smmvZsbl-QDBA6TLA2iEtd) | 03.04.2020 |
| Onsets and Frames | Onsets and Frames is an automatic music transcription framework with piano and drums models | <ul><li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Erich Elsen](https://github.com/ekelsen)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.11153), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.12247), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.00188)</li><li>[blog post](http://g.co/magenta/onsets-frames)</li><li>[data](https://g.co/magenta/maestro-wave2midi2wave), [data](https://magenta.tensorflow.org/datasets/e-gmd)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://goo.gl/magenta/onsets-frames-code)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/onsets_frames_transcription/onsets_frames_transcription.ipynb) | 02.04.2020 |
| Classification of chest vs. adominal X-rays | The goal of this tutorial is to build a deep learning classifier to accurately differentiate between chest and abdominal X-rays | [tmoneyx01](https://github.com/tmoneyx01) | <ul><li>[annotator](https://public.md.ai/annotator/project/PVq9raBJ)</li><li>[docs](https://docs.md.ai/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdai/mdai-client-py)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson1-xray-images-classification.ipynb) | 07.03.2020 |
| Lung X-Rays Semantic Segmentation | This lesson applies a U-Net for Semantic Segmentation of the lung fields on chest x-rays | [tmoneyx01](https://github.com/tmoneyx01) | <ul><li>[annotator](https://public.md.ai/annotator/project/aGq4k6NW)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1505.04597)</li><li>[data](https://ceb.nlm.nih.gov/repositories/tuberculosis-chest-x-ray-image-data-sets/)</li><li>[docs](https://docs.md.ai/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdai/mdai-client-py)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson2-lung-xrays-segmentation.ipynb) | 07.03.2020 |
| WikiArt (stylegan2) | Generation of paintings of different styles and genres | [Doron Adler](https://linktr.ee/Norod78) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.04958) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/Norod/my-colab-experiments/blob/master/WikiArt_Example_Generation_By_Peter_Baylies.ipynb) | 27.01.2020 |
| Earth Engine Python API and Folium Interactive Mapping | This notebook demonstrates how to setup the Earth Engine and provides several examples for visualizing Earth Engine processed data interactively using the folium library | [Qiusheng Wu](https://wetlands.io/) | <ul><li>[api](https://developers.google.com/earth-engine/python_install)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/python-visualization/folium)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/giswqs/qgis-earthengine-examples/blob/master/Folium/ee-api-folium-setup.ipynb) | 20.01.2020 |
| Train a GPT-2 Model on Tweets | Train the model on your downloaded tweets, and generate massive amounts of Tweets from it | [Max Woolf](https://minimaxir.com/) | <ul><li>[GPT-2](https://openai.com/blog/better-language-models/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/download-tweets-ai-text-gen)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1qxcQ2A1nNjFudAGN_mcMOnvV9sF_PkEb) | 16.01.2020 |
| Traffic counting | Making Road Traffic Counting App based on Computer Vision and OpenCV | [Andrey Nikishaev](https://github.com/creotiv) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/creotiv/object_detection_projects/tree/master/opencv_traffic_counting)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/machine-learning-world/tutorial-making-road-traffic-counting-app-based-on-computer-vision-and-opencv-166937911660)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=_o5iLbRHKao)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/12N4m_RYKqrpozRzh9qe7nQE_sIqQH9U8) | 10.01.2020 |
| Siamese NN | Implementation of Siamese Neural Networks built upon multihead attention mechanism for text semantic similarity task | [Tomasz Latkowski](https://github.com/tlatkowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.14599)</li><li>[data](https://nlp.stanford.edu/projects/snli/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tlatkowski/multihead-siamese-nets), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/anli/)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/quora-question-pairs)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tlatkowski/multihead-siamese-nets/blob/master/colab/multihead_siamese_nets.ipynb) | 19.12.2019 |
| Learning to Paint | Learning to Paint With Model-based Deep Reinforcement Learning | [Manuel Romero](https://mrm8488.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1903.04411)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/reinforcementlearning/comments/b5lpfl/learning_to_paint_with_modelbased_deep/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=YmOgKZ5oipk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/custom_learningtopaint.ipynb) | 17.12.2019 |
| Imaging-AMARETTO | An imaging genomics software tool to systematically interrogate multi-omics networks for relevance to radiography and histopathology imaging biomarkers of clinical outcomes with application to studies of brain tumors | <ul><li>[Nathalie Pochet](http://portals.broadinstitute.org/pochetlab/)</li> <li>[Olivier Gevaert](https://profiles.stanford.edu/olivier-gevaert)</li> <li>[Mohsen Nabian](https://github.com/monabiyan)</li> <li>[Celine Everaert](http://www.crig.ugent.be/en/node/510)</li> <li>[Jayendra Shinde](https://jayendrashinde91.github.io/)</li> <li>[Artur Manukyan](https://artur-man.github.io/)</li> <li>[Thorin Tabor](http://thorin.tabcreations.com/)</li> <li>[Mikel Hernaez](http://mikelhernaez.github.io/)</li> <li>[Jill Mesirov](https://en.wikipedia.org/wiki/Jill_P._Mesirov)</li></ul> | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/broadinstitute/ImagingAMARETTO)</li><li>[project](http://portals.broadinstitute.org/pochetlab/amaretto.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/14u1KZJ3Gf-9qjDycyBKzBiN5VzzOa2xU) | 29.11.2019 |
| Face toolbox | A collection of deep learning frameworks ported to Keras for face detection, face segmentation, face parsing, iris detection, and face verification | [shaoanlu](https://shaoanlu.github.io/) | [<img src="images/git.svg" alt="git" height=20/>](https://github.com/shaoanlu/face_toolbox_keras) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/face-toolbox-keras/blob/master/demo.ipynb) | 03.10.2019 |
| Generating Piano Music with Transformer | This Colab notebook lets you play with pretrained Transformer models for piano music generation, based on the Music Transformer | <ul><li>[Ian Simon](https://github.com/iansimon)</li> <li>[Anna Huang](https://github.com/czhuang)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.03762), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.04281)</li><li>[blog post](http://g.co/magenta/music-transformer)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb) | 16.09.2019 |
| Few-shot face translation | A GAN based approach for one model to swap them all: model is capable of producing faces that has its gaze direction, glasses, and hiar occlusions being consistent with given source face | [shaoanlu](https://shaoanlu.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1903.07291), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.01723)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/shaoanlu/fewshot-face-translation-GAN)</li><li>[link](https://drum.lib.umd.edu/handle/1903/21337)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/fewshot-face-translation-GAN/blob/master/colab_demo.ipynb) | 02.09.2019 |
| Waifu2x | This is the Google Colab implementation of tsurumeso's chainer implementation of waifu2x | [Margesh Phirke](https://github.com/mphirke) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1501.00092)</li><li>[demo](http://waifu2x.udp.jp/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mphirke/Google-Colab-waifu2x-chainer)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mphirke/waifu2x-chainer/blob/master/Waifu2x_Colab_Implementation.ipynb) | 23.08.2019 |
| GMCNN | Generative Multi-column Convolutional Neural Networks inpainting model in Keras | [Tomasz Latkowski](https://github.com/tlatkowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.08771)</li><li>[data](http://places2.csail.mit.edu/download.html), [data](https://nv-adlr.github.io/publication/partialconv-inpainting)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tlatkowski/inpainting-gmcnn-keras), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tlatkowski/inpainting-gmcnn-keras/blob/master/colab/Image_Inpainting_with_GMCNN_model.ipynb) | 09.08.2019 |
| AMARETTO | Multiscale and multimodal inference of regulatory networks to identify cell circuits and their drivers shared and distinct within and across biological systems of human disease | <ul><li>[Nathalie Pochet](http://portals.broadinstitute.org/pochetlab/)</li> <li>[Olivier Gevaert](https://profiles.stanford.edu/olivier-gevaert)</li> <li>[Mohsen Nabian](https://github.com/monabiyan)</li> <li>[Jayendra Shinde](https://jayendrashinde91.github.io/)</li> <li>[Celine Everaert](http://www.crig.ugent.be/en/node/510)</li> <li>[Thorin Tabor](http://thorin.tabcreations.com/)</li></ul> | <ul><li>[bioconductor](https://bioconductor.org/packages/release/bioc/html/AMARETTO.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/gevaertlab/AMARETTO)</li><li>[project](http://portals.broadinstitute.org/pochetlab/amaretto.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1JfnRoNgTVX_7VEGAAmjGjwP_yX2tdDxs) | 25.07.2019 |
| XLNet | Generalized Autoregressive Pretraining for Language Understanding | [Zhilin Yang](http://kimiyoung.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1906.08237)</li><li>[data](https://ai.stanford.edu/~amaas/data/sentiment/), [data](https://rajpurkar.github.io/SQuAD-explorer/), [data](https://www.cs.cmu.edu/~glai1/data/race/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/zihangdai/xlnet)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/zihangdai/xlnet/blob/master/notebooks/colab_imdb_gpu.ipynb) | 28.06.2019 |
| Breast Cancer detection | A Neural Network for detecting breast cancer in cell scans! | [Peter Teoh](https://github.com/tthtlc) | [blog post](http://www.laurencemoroney.com/easily-build-a-neural-net-for-breast-cancer-detection/) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/12DKmhi5z5Qx84iJQ8FTq5hHsG5UUHUcG) | 28.04.2019 |
| GazeML | Eye region landmarks detection | [shaoanlu](https://shaoanlu.github.io/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/shaoanlu/GazeML-keras)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=cLUHKYfZN5s)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/GazeML-keras/blob/master/demo_colab.ipynb) | 03.04.2019 |
| BERT with TPU | Using a free Colab Cloud TPU to fine-tune sentence and sentence-pair classification tasks built on top of pretrained BERT models and run predictions on tuned model | [Sourabh Bajaj](https://sourabhbajaj.com/) | <ul><li>[TPU quickstart](https://cloud.google.com/tpu/docs/quickstart)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/hub)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb) | 29.03.2019 |
| automl-gs on a TPU | Give an input CSV file and a target field you want to predict to automl-gs, and get a trained high-performing machine learning or deep learning model plus native Python code pipelines allowing you to integrate that model into any prediction workflow | [Max Woolf](https://minimaxir.com/) | [<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/automl-gs) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1sbF8cqnOsdzN9Bdt74eER5s_xXcdvatV) | 26.03.2019 |
| GANSynth | This notebook is a demo GANSynth, which generates audio with Generative Adversarial Networks | [Jesse Engel](https://github.com/jesseengel) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096)</li><li>[<img src="images/git.svg" alt="git" height=20/>](http://goo.gl/magenta/gansynth-code)</li><li>[project](https://storage.googleapis.com/magentadata/papers/gansynth/index.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/gansynth/gansynth_demo.ipynb) | 25.02.2019 |
| Edge Detection | Edge detection in OpenCV and skimage | [Yuhuang Hu](https://dgyblog.com/) | [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Edge_detection) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/10ZIvyVgDjGlWd09LJZzwboQs-4RPlCut) | 15.02.2019 |
| BERT on TF Hub | Predicting Movie Review Sentiment with BERT on TF Hub | [Dale Markowitz](https://daleonai.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google-research/bert)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb) | 12.02.2019 |
| Mask R-CNN | Code and visualizations to test, debug, and evaluate the Mask R-CNN model | <ul><li>[Jirka Borovec](https://github.com/Borda)</li> <li>[Waleed Abdulla](https://waleedka.github.io/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06870)</li><li>[blog post](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46)</li><li>[data](http://cocodataset.org/#home)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/matterport/Mask_RCNN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jremillard/images-to-osm), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/huuuuusy/Mask-RCNN-Shiny)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/data-science-bowl-2018)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/geoai/reconstructing-3d-buildings-from-aerial-lidar-with-ai-details-6a81cb3079c0)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=OOT3UIXZztE)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/matterport/Mask_RCNN/blob/master/samples/coco/inspect_model.ipynb) | 22.01.2019 |
| RSNA Pneumonia Detection Challenge (Kaggel API) | The basics of parsing the competition dataset, training using a detector basd on the Mask-RCNN algorithm for object detection and instance segmentation | [tmoneyx01](https://github.com/tmoneyx01) | <ul><li>[annotator](https://public.md.ai/annotator/project/LxR6zdR2/workspace)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06870)</li><li>[docs](https://docs.md.ai/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdai/mdai-client-py)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson3-rsna-pneumonia-detection-kaggle.ipynb) | 03.09.2018 |
| HoF | This notebook will walk you step by step through the process of using a pre-trained model to detect faces in an image | [Lucas Persona](http://www.lucaspersona.com/) | <ul><li>[data](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/the-house-of-black-and-white/hall-of-faces)</li><li>[yolo](https://pjreddie.com/darknet/yolo/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1lJWquGmKoMm68qNuwjSnfMjjIi-UTzI1) | 23.04.2018 |
| Group Normalization | A simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. | [shaoanlu](https://shaoanlu.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1803.08494)</li><li>[blog post](https://shaoanlu.wordpress.com/2018/03/26/experiment-with-group-normalization/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/shaoanlu/GroupNormalization-keras), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/zalandoresearch/fashion-mnist)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/GroupNormalization-keras/blob/master/group_norm_experiments.ipynb) | 26.03.2018 |
| Latent Constraints | Conditional Generation from Unconditional Generative Models | <ul><li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Matthew Hoffman](http://matthewdhoffman.com/)</li> <li>[Adam Roberts](https://github.com/adarob)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1711.05772)</li><li>[data](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/latent_constraints/latentconstraints.ipynb) | 27.11.2017 |
| Performance RNN | This notebook shows you how to generate new performed compositions from a trained model | <ul><li>[Ian Simon](https://github.com/iansimon)</li> <li>[Sageev Oore](https://github.com/osageev)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li></ul> | <ul><li>[blog post](https://magenta.tensorflow.org/performance-rnn)</li><li>[data](http://www.piano-e-competition.com/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/magenta/magenta/tree/master/magenta/models/performance_rnn)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/performance_rnn/performance_rnn.ipynb) | 11.07.2017 |
| NSynth | This colab notebook has everything you need to upload your own sounds and use NSynth models to reconstruct and interpolate between them | <ul><li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Cinjon Resnick](https://github.com/cinjon)</li> <li>[Adam Roberts](https://github.com/adarob)</li> <li>[Sander Dieleman](https://benanne.github.io/)</li> <li>[Karen Simonyan](https://scholar.google.co.uk/citations?user=L7lMQkQAAAAJ)</li> <li>[Mohammad Norouzi](https://norouzi.github.io/)</li> <li>[Douglas Eck](https://github.com/douglaseck)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1704.01279)</li><li>[blog post](https://magenta.tensorflow.org/nsynth)</li><li>[data](https://magenta.tensorflow.org/datasets/nsynth)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth)</li><li>[tutorial](https://magenta.tensorflow.org/nsynth-fastgen)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=AaALLWQmCdI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=BOoSy-Pg8is)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb) | 06.04.2017 |
## Tutorials
| name | description | authors | links | colaboratory | update |
|------|-------------|:--------|:------|:------------:|:------:|
| High-performance Simulation with Kubernetes | This tutorial will describe how to set up high-performance simulation using a TFF runtime running on Kubernetes | [Jason Roselander](https://github.com/roselander) | <ul><li>[GKE](https://cloud.google.com/kubernetes-engine/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[shell](https://cloud.google.com/shell/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/high_performance_simulation_with_kubernetes.ipynb) | 29.04.2022 |
| Building Your Own Federated Learning Algorithm | In this tutorial, we discuss how to implement federated learning algorithms without deferring to the tff.learning API | [Zachary Charles](https://zachcharles.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.08610)</li><li>[blog post](https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/building_your_own_federated_learning_algorithm.ipynb) | 26.04.2022 |
| Federated Learning for Text Generation | For this tutorial, we start with a RNN that generates ASCII characters, and refine it via federated learning | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1812.01097), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[data](http://www.ibiblio.org/pub/docs/books/gutenberg/9/98/98.txt), [data](http://www.ibiblio.org/pub/docs/books/gutenberg/4/46/46.txt)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/hub)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb) | 26.04.2022 |
| Custom Federated Algorithms, Part 1: Introduction to the Federated Core | This tutorial is the first part of a two-part series that demonstrates how to implement custom types of federated algorithms in TensorFlow Federated using the Federated Core - a set of lower-level interfaces that serve as a foundation upon which we have implemented the Federated Learning layer | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_core), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_1.ipynb) | 26.04.2022 |
| Custom Federated Algorithms, Part 2: Implementing Federated Averaging | This tutorial is the second part of a two-part series that demonstrates how to implement custom types of federated algorithms in TFF using the Federated Core, which serves as a foundation for the Federated Learning layer | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/learning/federated_averaging.py)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_core), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_2.ipynb) | 26.04.2022 |
| High-performance simulations with TFF | This tutorial will describe how to setup high-performance simulations with TFF in a variety of common scenarios | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/simulations.ipynb) | 26.04.2022 |
| Integrated gradients | This tutorial demonstrates how to implement Integrated Gradients, an Explainable AI technique | [Google](https://www.tensorflow.org/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.01365)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/integrated_gradients)</li><li>[visualizing](https://distill.pub/2020/attribution-baselines/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Linear_interpolation), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Riemann_sum)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/interpretability/integrated_gradients.ipynb) | 26.04.2022 |
| Simple audio recognition | This tutorial will show you how to build a basic speech recognition network that recognizes ten different words | [Google](https://www.tensorflow.org/) | <ul><li>[coursera](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/speech-recognition)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/speech_commands)</li><li>[tf.js](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb) | 26.04.2022 |
| Image segmentation | This tutorial focuses on the task of image segmentation, using a modified U-Net | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[data](https://www.robots.ox.ac.uk/~vgg/data/pets/)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/carvana-image-masking-challenge/overview)</li><li>[u-net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb) | 26.04.2022 |
| Introduction to the TensorFlow Models NLP library | You will learn how to build transformer-based models for common NLP tasks including pretraining, span labelling and classification using the building blocks from NLP modeling library | [Chen Chen](https://github.com/chenGitHuber) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/models/tree/master/official/nlp/modeling)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/nlp/nlp_modeling_library_intro.ipynb) | 22.04.2022 |
| Federated Learning for Image Classification | In this tutorial, we use the classic MNIST training example to introduce the Federated Learning API layer of TFF, tff.learning - a set of higher-level interfaces that can be used to perform common types of federated learning tasks, such as federated training, against user-supplied models implemented in TensorFlow | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[data](https://www.nist.gov/srd/nist-special-database-19)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning), [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/image-classification)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb) | 20.04.2022 |
| TFF for Federated Learning Research: Model and Update Compression | In this tutorial, we use the EMNIST dataset to demonstrate how to enable lossy compression algorithms to reduce communication cost in the Federated Averaging algorithm | [Weikang Song](https://github.com/swkpku) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[tensor encoding](http://jakubkonecny.com/files/tensor_encoding.pdf)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/emnist), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/api_docs/python/tff/learning/build_federated_averaging_process)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/tff_for_federated_learning_research_compression.ipynb) | 20.04.2022 |
| YOLOv5 on Custom Objects | This notebook shows training on your own custom objects | [Jacob Solawetz](https://blog.roboflow.com/author/jacob/) | <ul><li>[blog post](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/)</li><li>[data](https://public.roboflow.ai/object-detection/bccd)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ultralytics/yolov5)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1gDZ2xcTOgR39tGGs-EZ6i3RTs16wmzZQ) | 05.04.2022 |
| TorchGeo | PyTorch domain library that provides datasets, transforms, samplers, and pre-trained models specific to geospatial data | <ul><li>[Adam Stewart](https://github.com/adamjstewart)</li> <li>[Caleb Robinson](https://calebrob.com/)</li> <li>[Isaac Corley](https://github.com/isaaccorley)</li> <li>[Anthony Ortiz](https://github.com/anthonymlortiz)</li> <li>[Juan Lavista Ferres](https://www.microsoft.com/en-us/research/people/jlavista/)</li> <li>[Arindam Banerjee](https://arindam.cs.illinois.edu/)</li></ul> | <ul><li>[NDBI](https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri/)</li><li>[NDVI](https://gisgeography.com/ndvi-normalized-difference-vegetation-index/)</li><li>[NDWI](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndwi/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.08872)</li><li>[data](https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l2a/), [data](https://www.cogeo.org/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/torchgeo), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/davemlz/awesome-spectral-indices)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/microsoft/torchgeo/blob/main/docs/tutorials/indices.ipynb) | 05.04.2022 |
| Autoencoders | This tutorial introduces autoencoders with three examples: the basics, image denoising, and anomaly detection | [Google](https://www.tensorflow.org/) | <ul><li>[blog post](https://blog.keras.io/building-autoencoders-in-keras.html)</li><li>[book](https://www.deeplearningbook.org/contents/autoencoders.html)</li><li>[data](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000)</li><li>[examples](https://anomagram.fastforwardlabs.com/#/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/autoencoder)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb) | 05.04.2022 |
| Transformer | This tutorial trains a Transformer model to translate Portuguese to English | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.03762), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1903.03878)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/neulab/word-embeddings-for-nmt)</li><li>[link](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb) | 25.03.2022 |
| highway-env | A collection of environments for autonomous driving and tactical decision-making tasks | [Edouard Leurent](https://edouardleurent.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.03483), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.05701), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2101.07140)</li><li>[docs](https://highway-env.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/eleurent/highway-env), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/eleurent/rl-agents), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/eleurent/finite-mdp), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/baselines/tree/master/baselines/her)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/eleurent/highway-env/blob/master/scripts/parking_model_based.ipynb) | 19.03.2022 |
| Text classification with RNN | This text classification tutorial trains a recurrent neural network on the IMDB large movie review dataset for sentiment analysis | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[data](http://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[link](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-classification)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb) | 17.03.2022 |
| Real-Time Voice Cloning | SV2TTS with a vocoder that works in real-time | <ul><li>[Corentin Jemine](https://github.com/CorentinJ)</li> <li>[Erdene-Ochir Tuguldur](https://github.com/tugstugi)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1806.04558), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1802.08435), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.10135), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10467)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CorentinJ/Real-Time-Voice-Cloning), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/fatchord/WaveRNN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/coqui-ai/tts), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/resemble-ai/Resemblyzer)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-O_hYhToKoA)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/RealTimeVoiceCloning.ipynb) | 07.03.2022 |
| Silero Models | Pre-trained speech-to-text, text-to-speech and text-enhancement models made embarrassingly simple | [Silero team](https://www.silero.ai/about/) | <ul><li>[STT](https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/), [STT](https://thegradient.pub/a-speech-to-text-practitioners-criticisms-of-industry-and-academia/), [STT](https://habr.com/ru/post/519562/)</li><li>[TTS](https://habr.com/ru/post/660571/), [TTS](https://habr.com/ru/post/549482/)</li><li>[Text Enhancement](https://habr.com/ru/post/581960/)</li><li>[VAD](https://thegradient.pub/one-voice-detector-to-rule-them-all/), [VAD](https://habr.com/ru/post/537276/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/snakers4/silero-models)</li><li>[site](https://www.silero.ai/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb) | 27.02.2022 |
| NeMo | A conversational AI toolkit built for researchers working on automatic speech recognition, natural language processing, and text-to-speech synthesis | <ul><li>[Oleksii Kuchaiev](http://kuchaev.com/)</li> <li>[Jason Li](https://scholar.google.com/citations?user=V28bxDwAAAAJ)</li> <li>[Chip Huyen](https://huyenchip.com/)</li> <li>[Oleksii Hrinchuk](https://github.com/AlexGrinch)</li> <li>[Ryan Leary](https://github.com/ryanleary)</li> <li>[Boris Ginsburg](https://github.com/borisgin)</li> <li>[Samuel Kriman](https://github.com/sam1373)</li> <li>[Stanislav Beliaev](https://github.com/stasbel)</li> <li>[Vitaly Lavrukhin](https://github.com/vsl9)</li> <li>[Jack Cook](https://jackcook.com/)</li></ul> | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/NeMo)</li><li>[project](https://docs.nvidia.com/deeplearning/nemo/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/wBgpMf_KQVw)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/NVIDIA/NeMo/blob/master/tutorials/00_NeMo_Primer.ipynb) | 23.02.2022 |
| Classify text with BERT | This tutorial contains complete code to fine-tune BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews | [Google](https://www.tensorflow.org/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1711.05101)</li><li>[data](https://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-classification)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://tfhub.dev/google/collections/bert/1)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb) | 22.02.2022 |
| NMT with attention | This notebook trains a seq2seq model for Spanish to English translation | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1409.0473), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.04025v5)</li><li>[data](http://www.manythings.org/anki/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Neural_machine_translation)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb) | 22.02.2022 |
| GLUE using BERT on TPU | This tutorial contains complete end-to-end code to train models on a TPU | [Google](https://www.tensorflow.org/) | <ul><li>[GLUE](https://gluebenchmark.com/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/guide/tpu)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb) | 22.02.2022 |
| Data augmentation | This tutorial demonstrates data augmentation: a technique to increase the diversity of your training set by applying random transformations such as image rotation | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/data-augmentation)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/tf_flowers)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Data_augmentation)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb) | 22.02.2022 |
| Text generation with RNN | This tutorial demonstrates how to generate text using a character-based RNN | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-generation)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb) | 25.01.2022 |
| Word2Vec | Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets | [Google](https://www.tensorflow.org/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1301.3781)</li><li>[link](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/cbow-word2vec), [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/skip-gram-word2vec)</li><li>[projector](http://projector.tensorflow.org/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Zipf%27s_law)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb) | 24.01.2022 |
| Word embeddings | This tutorial contains an introduction to word embeddings | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[data](http://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[projector](http://projector.tensorflow.org/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb) | 15.01.2022 |
| DeepDream | This tutorial contains a minimal implementation of DeepDream: an experiment that visualizes the patterns learned by a neural network | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1409.4842)</li><li>[blog post](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Inception)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb) | 13.01.2022 |
| Transfer learning and fine-tuning | You will learn how to classify images of cats and dogs by using transfer learning from a pre-trained network | [François Chollet](https://fchollet.com/) | <ul><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/transfer-learning)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Transfer_learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb) | 11.01.2022 |
| MLP | The most basic neural network architectures, a multilayer perceptron, also known as a feedforward network | [Ben Trevett](https://bentrevett.com/) | <ul><li>[NN and DL](http://neuralnetworksanddeeplearning.com/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1702.03118), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.12943), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.04020)</li><li>[optimization](https://ruder.io/optimizing-gradient-descent/)</li><li>[<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-only), [<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/vision/stable/transforms.html#transforms-on-torch-tensor-only)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Multilayer_perceptron)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb) | 26.12.2021 |
| AlexNet | A neural network model that uses convolutional neural network layers and was designed for the ImageNet challenge | [Ben Trevett](https://bentrevett.com/) | <ul><li>[ILSVRC](https://image-net.org/challenges/LSVRC/)</li><li>[LR](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html)</li><li>[PMLR](https://proceedings.mlr.press/v9/glorot10a.html)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1409.0575)</li><li>[cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html)</li><li>[dropout](https://sebastianraschka.com/faq/docs/dropout-activation.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/davidtvs/pytorch-lr-finder)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/alexnet)</li><li>[<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/vision/stable/models.html)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Regularization_(mathematics)), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/AlexNet)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb) | 26.12.2021 |
| LeNet | A neural network model that uses convolutional neural network layers and was designed for classifying handwritten characters | [Ben Trevett](https://bentrevett.com/) | <ul><li>[CNN](https://cs231n.github.io/convolutional-networks/)</li><li>[LeNet-5](http://yann.lecun.com/exdb/lenet/)</li><li>[guide](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)</li><li>[paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/lenet)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Convolution), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Sobel_operator), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Gaussian_blur)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/2_lenet.ipynb) | 26.12.2021 |
| FLAML | Lightweight Python library that finds accurate machine learning models automatically, efficiently and economically | <ul><li>[Chi Wang](https://github.com/sonichi)</li> <li>[Qingyun Wu](https://qingyun-wu.github.io/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.04815), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.01571)</li><li>[docs](https://microsoft.github.io/FLAML/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/FLAML)</li><li>[paper](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/channel/UCfU0zfFXHXdAd5x-WvFBk5A), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/euXpDYGgkGM)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/microsoft/FLAML/blob/master/notebook/flaml_automl.ipynb) | 17.12.2021 |
| Image captioning | Given an image our goal is to generate a caption | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1502.03044)</li><li>[data](https://cocodataset.org/#home)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb) | 14.12.2021 |
| Image classification | This tutorial shows how to classify images of flowers | [Billy Lamberta](https://github.com/lamberta) | [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/image-classification) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb) | 28.11.2021 |
| ruGPT3 | Example of inference of RuGPT3XL | [Anton Emelyanov](https://github.com/king-menin) | <ul><li>[cristofari](https://sbercloud.ru/ru/christofari)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sberbank-ai/ru-gpts), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/DeepSpeedExamples/tree/master/Megatron-LM)</li><li>[huggingface](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate)</li><li>[sparse attention](https://www.deepspeed.ai/tutorials/sparse-attention/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/ruGPT3XL_generation.ipynb) | 17.11.2021 |
| CompilerGym | A reinforcement learning toolkit for compiler optimizations | <ul><li>[Chris Cummins](https://chriscummins.cc/)</li> <li>[Bram Wasti](https://github.com/bwasti)</li> <li>[Jiadong Guo](https://jd-eth.github.io/)</li> <li>[Brandon Cui](https://www.linkedin.com/in/bcui19/)</li> <li>[Jason Ansel](https://jasonansel.com/)</li> <li>[Sahir Gomez](https://github.com/sahirgomez1)</li> <li>[Olivier Teytaud](https://github.com/teytaud)</li> <li>[Benoit Steiner](http://bsteiner.info/)</li> <li>[Yuandong Tian](http://yuandong-tian.com/)</li> <li>[Hugh Leather](https://github.com/hughleat)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.08267)</li><li>[docs](https://facebookresearch.github.io/CompilerGym/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/CompilerGym)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/facebookresearch/CompilerGym/blob/development/examples/getting-started.ipynb) | 16.11.2021 |
| Pix2Pix | This notebook demonstrates image to image translation using conditional GAN's | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1611.07004)</li><li>[data](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb) | 10.11.2021 |
| Detectron2 | FAIR's next-generation platform for object detection and segmentation | [Yuxin Wu](http://ppwwyyxx.com/) | <ul><li>[blog post](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/)</li><li>[docs](https://detectron2.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/detectron2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) | 05.11.2021 |
| DeepStyle | The Neural Style algorithm synthesizes a pastiche by separating and combining the content of one image with the style of another image using convolutional neural networks | <ul><li>[Cameron Smith](https://github.com/cysmith)</li> <li>[Alexander Spirin](https://github.com/Sxela)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1604.08610), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1606.05897), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.06576)</li><li>[cvpr](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/cysmith/neural-style-tf)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Pastiche), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/The_Starry_Night), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/YUV), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Lab_color_space), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/YCbCr), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/CIELUV), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Pareidolia)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/14aJ7HQPbcP0sNRIY-FRO4u6lxtlyyxI_) | 01.10.2021 |
| EfficientNetV2 | A family of image classification models, which achieve better parameter efficiency and faster training speed than prior arts | <ul><li>[Mingxing Tan](https://scholar.google.com/citations?user=6POeyBoAAAAJ)</li> <li>[Quoc V. Le](https://cs.stanford.edu/~quocle/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.00298), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.11946)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/automl/tree/master/efficientnetv2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/TensorRT/tree/master/samples/python/efficientnet)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/google/automl/blob/master/efficientnetv2/tutorial.ipynb) | 24.09.2021 |
| Droidlet | A modular embodied agent architecture and platform for building embodied agents | <ul><li>[Anurag Pratik](https://github.com/anuragprat1k)</li> <li>[Soumith Chintala](https://soumith.ch/)</li> <li>[Kavya Srinet](https://github.com/kavyasrinet)</li> <li>[Dhiraj Gandhi](https://dhiraj100892.github.io/)</li> <li>[Rebecca Qian](https://github.com/Rebecca-Qian)</li> <li>[Yuxuan Sun](https://github.com/snyxan)</li> <li>[Ryan Drew](https://rdrew.dev/)</li> <li>[Sara Elkafrawy](https://github.com/saraEbrahim)</li> <li>[Anoushka Tiwari](https://www.linkedin.com/in/anoushka-tiwari)</li> <li>[Tucker Hart](https://www.linkedin.com/in/tucker-hart-05a638133)</li> <li>[Mary Williamson](https://scholar.google.com/citations?user=Ys4xB-QAAAAJ)</li> <li>[Abhinav Gupta](http://www.cs.cmu.edu/~abhinavg/)</li> <li>[Arthur Szlam](https://scholar.google.com/citations?user=u3-FxUgAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2101.10384), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.08584)</li><li>[docs](https://facebookresearch.github.io/droidlet/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/droidlet)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/facebookresearch/droidlet/blob/master/examples_and_tutorials/tutorials/droidlet_for_physical_robots.ipynb) | 15.09.2021 |
| GPT-J-6B | A 6 billion parameter, autoregressive text generation model trained on The Pile | <ul><li>[Ben Wang](https://benwang.dev/)</li> <li>[Aran Komatsuzaki](https://arankomatsuzaki.wordpress.com/about-me/)</li> <li>[Janko Prester](https://www.jankoprester.com/)</li></ul> | <ul><li>[The Pile](https://pile.eleuther.ai/)</li><li>[blog post](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/kingoflolz/mesh-transformer-jax), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/EleutherAI/gpt-neox), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/DeepSpeed)</li><li>[web demo](https://6b.eleuther.ai/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb) | 15.09.2021 |
| Sentence Transformers | Multilingual Sentence, Paragraph, and Image Embeddings using BERT & Co | <ul><li>[Nils Reimers](https://www.nils-reimers.de/)</li> <li>[Iryna Gurevych](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1908.10084), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.09813), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.08240)</li><li>[docs](https://www.sbert.net/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/UKPLab/sentence-transformers)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb) | 13.09.2021 |
| Actor-Critic | This tutorial demonstrates how to implement the Actor-Critic method using TensorFlow to train an agent on the Open AI Gym CartPole-V0 environment | [Mark Daoust](https://github.com/MarkDaoust) | <ul><li>[gym](https://gym.openai.com/)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf), [<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Temporal_difference_learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb) | 08.09.2021 |
| Lucid Sonic Dreams | Syncs GAN-generated visuals to music | [Mikael Alafriz](https://github.com/mikaelalafriz) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mikaelalafriz/lucid-sonic-dreams), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/justinpinkney/awesome-pretrained-stylegan2)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://towardsdatascience.com/introducing-lucid-sonic-dreams-sync-gan-art-to-music-with-a-few-lines-of-python-code-b04f88722de1)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/l-nGC-ve7sI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1Y5i50xSFIuN3V4Md8TB30_GOAtts7RQD) | 24.08.2021 |
| The Autodiff Cookbook | You'll go through a whole bunch of neat autodiff ideas that you can cherry pick for your own work, starting with the basics | <ul><li>[Alex Wiltschko](https://github.com/alexbw)</li> <li>[Matthew Johnson](http://people.csail.mit.edu/mattjj/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1406.2572), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.04454), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1802.03451), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1811.07062)</li><li>[book](https://mitpress.mit.edu/sites/default/files/titles/content/sicm_edition_2/book.html), [book](https://mitpress.mit.edu/books/functional-differential-geometry)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/jax/issues/446#issuecomment-467105048), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/jax#auto-vectorization-with-vmap), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hips/autograd)</li><li>[tutorial](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Truncated_Newton_method), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Pullback_(differential_geometry)), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Holomorphic_function), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/autodiff_cookbook.ipynb) | 04.08.2021 |
| Neural style transfer | This tutorial uses deep learning to compose one image in the style of another image | [Billy Lamberta](https://github.com/lamberta) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.06576) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb) | 15.06.2021 |
| CycleGAN | This notebook demonstrates unpaired image to image translation using conditional GAN's | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.10593)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/cycle_gan)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb) | 02.06.2021 |
| CNN | This tutorial demonstrates training a simple Convolutional Neural Network to classify CIFAR images | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[cifar](https://www.cs.toronto.edu/~kriz/cifar.html)</li><li>[link](https://developers.google.com/machine-learning/glossary/#convolutional_neural_network)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb) | 21.05.2021 |
| Custom GPT-2 + Tokenizer | Train a custom GPT-2 model for free on a GPU using aitextgen! | [Max Woolf](https://minimaxir.com/) | <ul><li>[data](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)</li><li>[docs](https://docs.aitextgen.io/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/aitextgen)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/144MdX5aLqrQ3-YW-po81CQMrD6kpgpYh) | 17.05.2021 |
| Train a GPT-2 Text-Generating Model | Retrain an advanced text generating neural network on any text dataset for free on a GPU using Colaboratory using aitextgen! | [Max Woolf](https://minimaxir.com/) | <ul><li>[data](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)</li><li>[docs](https://docs.aitextgen.io/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/aitextgen)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-generation)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD) | 17.05.2021 |
| EasyNMT | Easy to use, state-of-the-art machine translation for more than 100+ languages | [Nils Reimers](https://www.nils-reimers.de/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00401), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.11125)</li><li>[demo](http://easynmt.net/demo/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/UKPLab/EasyNMT), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Helsinki-NLP/Opus-MT), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/pytorch/fairseq/tree/master/examples/multilingual)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1X47vgSiOphpxS5w_LPtjQgJmiSTNfRNC) | 26.04.2021 |
| Deep-MAC | Welcome to the Novel class segmentation demo | [Vighnesh Birodkar](http://vighneshbirodkar.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.00613)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/deep-mac)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/deepmac_colab.ipynb) | 02.04.2021 |
| GPT Neo | An implementation of model & data parallel GPT2 & GPT3 -like models, with the ability to scale up to full GPT3 sizes (and possibly more!), using the mesh-tensorflow library | [EleutherAI](https://www.eleuther.ai/) | <ul><li>[GPT-2](https://openai.com/blog/better-language-models/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.14165), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.05150), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1701.06538)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/EleutherAI/gpt-neo), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/mesh), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/EleutherAI/gpt-neox/)</li><li>[pretrained](https://the-eye.eu/public/AI/gptneo-release/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb) | 28.03.2021 |
| CVAE | This notebook demonstrates how train a Variational Autoencoder on the MNIST dataset | [Billy Lamberta](https://github.com/lamberta) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1312.6114), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1401.4082) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cvae.ipynb) | 22.03.2021 |
| DCGAN | This tutorial demonstrates how to generate images of handwritten digits using a Deep Convolutional Generative Adversarial Network | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1511.06434), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1701.00160)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/jessicali9530/celeba-dataset)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb) | 12.03.2021 |
| Adversarial FGSM | This tutorial creates an adversarial example using the Fast Gradient Signed Method attack. This was one of the first and most popular attacks to fool a neural network. | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1412.6572)</li><li>[imagenet](http://www.image-net.org/)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications/MobileNetV2)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb) | 12.03.2021 |
| GAN steerability | We will navigate in GAN latent space to simulate various camera transformations | <ul><li>[Ali Jahanian](http://people.csail.mit.edu/jahanian/)</li> <li>[Lucy Chai](http://people.csail.mit.edu/lrchai/)</li> <li>[Phillip Isola](http://web.mit.edu/phillipi/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.07171), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ali-design/gan_steerability)</li><li>[project](https://ali-design.github.io/gan_steerability/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/nS0V64sF7Cw)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1kn6yG8PqD1U2bUcy32V1iAVjzlcQWcG3) | 04.03.2021 |
| TF-Ranking | This tutorial is an end-to-end walkthrough of training a TensorFlow Ranking neural network model which incorporates sparse textual features | [Rama Kumar](https://github.com/ramakumar1729) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.09676), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1812.00073), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.08957), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1811.04415)</li><li>[data](http://hamedz.ir/resources/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/ranking), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/input.proto#L72)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Mean_reciprocal_rank), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb) | 04.02.2021 |
| TensorNetwork | A library for easy and efficient manipulation of tensor networks | [Chase Roberts](http://thenerdstation.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1708.00006), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1306.2164)</li><li>[docs](https://tensornetwork.readthedocs.io/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/TensorNetwork)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=YN2YBB0viKo)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/google/TensorNetwork/blob/master/colabs/Tensor_Networks_in_Neural_Networks.ipynb) | 21.01.2021 |
| Spleeter | Deezer source separation library including pretrained models | <ul><li>[Romain Hennequin](http://romain-hennequin.fr/)</li> <li>[Anis Khlif](https://github.com/alreadytaikeune)</li> <li>[Félix Voituret](https://github.com/Faylixe)</li> <li>[Manuel Moussallam](https://mmoussallam.github.io/)</li></ul> | <ul><li>[blog post](https://deezer.io/releasing-spleeter-deezer-r-d-source-separation-engine-2b88985e797e)</li><li>[data](https://sigsep.github.io/datasets/musdb.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deezer/spleeter)</li><li>[project](https://research.deezer.com/projects/spleeter.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb) | 10.01.2021 |
| Semantic Segmentation | Pytorch implementation for Semantic Segmentation/Scene Parsing on MIT ADE20K dataset | <ul><li>[Bolei Zhou](https://boleizhou.github.io/)</li> <li>[Hang Zhao](https://hangzhaomit.github.io/)</li> <li>[Xavier Puig](https://people.csail.mit.edu/xavierpuig/)</li> <li>[Sanja Fidler](http://www.cs.toronto.edu/~fidler/index.html)</li> <li>[Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1608.05442), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1612.01105), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1807.10221), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.04514)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CSAILVision/semantic-segmentation-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/CSAILVision/sceneparsing), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hszhao/semseg)</li><li>[project](http://sceneparsing.csail.mit.edu/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb) | 21.08.2020 |
| CoVoST | A Large-Scale Multilingual Speech-To-Text Translation Corpus | <ul><li>[Changhan Wang](https://github.com/kahne)</li> <li>[Juan Pino](https://scholar.google.com/citations?user=weU_-4IAAAAJ)</li> <li>[Jiatao Gu](http://jiataogu.me/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2002.01320), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/pdf/2007.10310), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.06670)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/covost), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/kahne/SpeechTransProgress), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/pytorch/fairseq/tree/main/examples/speech_to_text), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/vizseq)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/11GK7k7G1CG1qHbdA9Pz1RtQ3vlCkuohV) | 07.08.2020 |
| Eager Few Shot Object Detection | Fine tuning of a RetinaNet architecture on very few examples of a novel class after initializing from a pre-trained COCO checkpoint | [kmindspark](https://github.com/kmindspark) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1708.02002)</li><li>[data](https://cocodataset.org/#explore)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/few-shot-object-detection)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb) | 11.07.2020 |
| YOLOv4 | This tutorial will help you build YOLOv4 easily in the cloud with GPU enabled so that you can run object detections in milliseconds! | [Aleksey Bochkovskiy](http://www.alexeyab.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.10934), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.08036)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/AlexeyAB/darknet/wiki)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://alexeyab84.medium.com/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe), [<img src="images/medium.svg" alt="medium" height=20/>](https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982)</li><li>[project](https://pjreddie.com/darknet/)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/MachineLearning/comments/gydxzd/p_yolov4_the_most_accurate_realtime_neural/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/1_SiUOYUoOI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/YDFf-TqJOFE)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1_GdoqCJWXsChrOiY8sZMr_zbr_fH-0Fg) | 25.06.2020 |
| Context R-CNN Demo | This notebook will walk you step by step through the process of using a pre-trained model to build up a contextual memory bank for a set of images, and then detect objects in those images+context using Context R-CNN | [pkulzc](https://github.com/pkulzc) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.03538)</li><li>[data](https://lila.science/datasets/snapshot-serengeti)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/context_rcnn_tutorial.ipynb) | 17.06.2020 |
| GAN Dissection | Visualizing and Understanding Generative Adversarial Networks | <ul><li>[David Bau](https://people.csail.mit.edu/davidbau/home/)</li> <li>[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)</li> <li>[Hendrik Strobelt](http://hendrik.strobelt.com/)</li> <li>[Bolei Zhou](http://bzhou.ie.cuhk.edu.hk/)</li> <li>[Joshua Tenenbaum](https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/)</li> <li>[William Freeman](https://billf.mit.edu/)</li> <li>[Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1811.10597), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1901.09887), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1807.10221)</li><li>[demo](http://gandissect.res.ibm.com/ganpaint.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CSAILVision/GANDissect), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/CSAILVision/NetDissect), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/junyanz/iGAN)</li><li>[project](https://gandissect.csail.mit.edu/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=yVCgUYe4JTM)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/SIDN-IAP/global-model-repr/blob/master/notebooks/gandissect_solutions.ipynb) | 04.05.2020 |
| Imagededup | This package provides functionality to make use of hashing algorithms that are particularly good at finding exact duplicates as well as convolutional neural networks which are also adept at finding near duplicates | <ul><li>[Tanuj Jain](https://github.com/tanujjain)</li> <li>[Christopher Lennan](https://github.com/clennan)</li> <li>[Dat Tran](https://dat-tran.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1704.04861)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/idealo/imagededup)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://fullstackml.com/wavelet-image-hash-in-python-3504fdd282b5)</li><li>[project](https://idealo.github.io/imagededup/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/idealo/imagededup/blob/master/examples/CIFAR10_duplicates.ipynb) | 03.10.2019 |
| Transfer Learning in NLP | This notebook accompanies the tutorial given at NAACL 2019 on Transfer Learning in Natural Language Processing | <ul><li>[Sebastian Ruder](https://ruder.io/)</li> <li>[Swabha Swayamdipta](https://swabhs.com/)</li> <li>[Thomas Wolf](https://thomwolf.io/)</li></ul> | <ul><li>[data](https://cogcomp.seas.upenn.edu/Data/QA/QC/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/huggingface/naacl_transfer_learning_tutorial), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/HazyResearch/metal)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/transfer-learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1iDHCYIrWswIKp-n-pOg69xLoZO09MEgf) | 03.06.2019 |
| RSNA Pneumonia Detection Challenge (MD.ai API) | The basics of parsing the competition dataset, training using a detector basd on the Mask-RCNN algorithm for object detection and instance segmentation | [tmoneyx01](https://github.com/tmoneyx01) | <ul><li>[annotator](https://public.md.ai/annotator/project/LxR6zdR2/workspace)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06870)</li><li>[docs](https://docs.md.ai/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdai/mdai-client-py)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson3-rsna-pneumonia-detection-mdai-client-lib.ipynb) | 29.08.2018 |
| Python Data Science Handbook | Jupyter notebook version of the Python Data Science Handbook by Jake VanderPlas | [Jake Vanderplas](http://vanderplas.com/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jakevdp/PythonDataScienceHandbook)</li><li>[project](https://jakevdp.github.io/PythonDataScienceHandbook/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb) | 14.08.2017 |

(generated by [generate_markdown.py](generate_markdown.py) based on [research.json](data/research.json) and [tutorials.json](data/tutorials.json))